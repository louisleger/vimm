{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ultralytics and dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.23M/6.23M [00:00<00:00, 51.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = ultralytics.YOLO(\"yolov8n.pt\")\n",
    "pt_model = model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([1, 84, 6300]), 3, torch.Size([1, 144, 20, 15]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 640, 480)\n",
    "dummy_output = pt_model(dummy_input)\n",
    "len(dummy_output), dummy_output[0].shape, len(dummy_output[1]), dummy_output[1][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.217529773712158. Dividing input by 255.\n",
      "0: 640x480 (no detections), 90.7ms\n",
      "Speed: 0.5ms preprocess, 90.7ms inference, 11.1ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([], device='cuda:0')\n",
       "conf: tensor([], device='cuda:0')\n",
       "data: tensor([], device='cuda:0', size=(0, 6))\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (640, 480)\n",
       "shape: torch.Size([0, 6])\n",
       "xywh: tensor([], device='cuda:0', size=(0, 4))\n",
       "xywhn: tensor([], device='cuda:0', size=(0, 4))\n",
       "xyxy: tensor([], device='cuda:0', size=(0, 4))\n",
       "xyxyn: tensor([], device='cuda:0', size=(0, 4))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dummy_input)[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame-data/0/ .DS_Store\n",
      "frame-data/0/ frame000000.png\n",
      "frame-data/0/ frame000001.png\n",
      "frame-data/0/ frame000002.png\n",
      "frame-data/0/ frame000003.png\n",
      "frame-data/0/ frame000004.png\n",
      "frame-data/0/ frame000005.png\n",
      "frame-data/0/ frame000006.png\n",
      "frame-data/0/ frame000007.png\n",
      "frame-data/0/ frame000008.png\n",
      "frame-data/0/ frame000009.png\n",
      "frame-data/0/ frame000010.png\n",
      "frame-data/0/ frame000011.png\n",
      "frame-data/0/ frame000012.png\n",
      "frame-data/0/ frame000013.png\n",
      "frame-data/0/ frame000014.png\n",
      "frame-data/0/ frame000015.png\n",
      "frame-data/0/ frame000016.png\n",
      "frame-data/0/ frame000017.png\n",
      "frame-data/0/ frame000018.png\n",
      "frame-data/0/ frame000019.png\n",
      "frame-data/0/ frame000020.png\n",
      "frame-data/0/ frame000021.png\n",
      "frame-data/0/ frame000022.png\n",
      "frame-data/0/ frame000023.png\n",
      "frame-data/0/ frame000024.png\n",
      "frame-data/0/ frame000025.png\n",
      "frame-data/0/ frame000026.png\n",
      "frame-data/0/ frame000027.png\n",
      "frame-data/0/ frame000028.png\n",
      "frame-data/0/ frame000029.png\n",
      "frame-data/0/ frame000030.png\n",
      "frame-data/0/ frame000031.png\n",
      "frame-data/0/ frame000032.png\n",
      "frame-data/0/ frame000033.png\n",
      "frame-data/0/ frame000034.png\n",
      "frame-data/0/ frame000035.png\n",
      "frame-data/0/ frame000036.png\n",
      "frame-data/0/ frame000037.png\n",
      "frame-data/0/ frame000038.png\n",
      "frame-data/0/ frame000039.png\n",
      "frame-data/0/ frame000040.png\n",
      "frame-data/0/ frame000041.png\n",
      "frame-data/0/ frame000042.png\n",
      "frame-data/0/ frame000043.png\n",
      "frame-data/0/ frame000044.png\n",
      "frame-data/0/ frame000045.png\n",
      "frame-data/0/ frame000046.png\n",
      "frame-data/0/ frame000047.png\n",
      "frame-data/0/ frame000048.png\n",
      "frame-data/0/ frame000049.png\n",
      "frame-data/0/ frame000050.png\n",
      "frame-data/0/ frame000051.png\n",
      "frame-data/0/ frame000052.png\n",
      "frame-data/0/ frame000053.png\n",
      "frame-data/0/ frame000054.png\n",
      "frame-data/0/ frame000055.png\n",
      "frame-data/0/ frame000056.png\n",
      "frame-data/0/ frame000057.png\n",
      "frame-data/0/ frame000058.png\n",
      "frame-data/0/ frame000059.png\n",
      "frame-data/0/ frame000060.png\n",
      "frame-data/0/ frame000061.png\n",
      "frame-data/0/ frame000062.png\n",
      "frame-data/0/ frame000063.png\n",
      "frame-data/0/ frame000064.png\n",
      "frame-data/0/ frame000065.png\n",
      "frame-data/0/ frame000066.png\n",
      "frame-data/0/ frame000067.png\n",
      "frame-data/0/ frame000068.png\n",
      "frame-data/0/ frame000069.png\n",
      "frame-data/0/ frame000070.png\n",
      "frame-data/0/ frame000071.png\n",
      "frame-data/0/ frame000072.png\n",
      "frame-data/0/ frame000073.png\n",
      "frame-data/0/ frame000074.png\n",
      "frame-data/0/ frame000075.png\n",
      "frame-data/0/ frame000076.png\n",
      "frame-data/0/ frame000077.png\n",
      "frame-data/0/ frame000078.png\n",
      "frame-data/0/ frame000079.png\n",
      "frame-data/0/ frame000080.png\n",
      "frame-data/0/ frame000081.png\n",
      "frame-data/0/ frame000082.png\n",
      "frame-data/0/ frame000083.png\n",
      "frame-data/0/ frame000084.png\n",
      "frame-data/0/ frame000085.png\n",
      "frame-data/0/ frame000086.png\n",
      "frame-data/0/ frame000087.png\n",
      "frame-data/0/ frame000088.png\n",
      "frame-data/0/ frame000089.png\n",
      "frame-data/0/ frame000090.png\n",
      "frame-data/0/ frame000091.png\n",
      "frame-data/0/ frame000092.png\n",
      "frame-data/0/ frame000093.png\n",
      "frame-data/0/ frame000094.png\n",
      "frame-data/0/ frame000095.png\n",
      "frame-data/0/ frame000096.png\n",
      "frame-data/0/ frame000097.png\n",
      "frame-data/0/ frame000098.png\n",
      "frame-data/0/ frame000099.png\n",
      "frame-data/0/ frame000100.png\n",
      "frame-data/0/ frame000101.png\n",
      "frame-data/0/ frame000102.png\n",
      "frame-data/0/ frame000103.png\n",
      "frame-data/0/ frame000104.png\n",
      "frame-data/0/ frame000105.png\n",
      "frame-data/0/ frame000106.png\n",
      "frame-data/0/ frame000107.png\n",
      "frame-data/0/ frame000108.png\n",
      "frame-data/0/ frame000109.png\n",
      "frame-data/0/ frame000110.png\n",
      "['']\n",
      "['0 0.118078 0.538500 0.023500 0.028500', '']\n",
      "['0 0.117555 0.541354 0.020297 0.022792', '']\n",
      "['0 0.130898 0.570542 0.042734 0.038458', '']\n",
      "['0 0.140625 0.590635 0.044875 0.039896', '']\n",
      "['0 0.157078 0.555594 0.048062 0.039896', '']\n",
      "['0 0.187633 0.540063 0.053422 0.052708', '']\n",
      "['0 0.195109 0.509437 0.051281 0.048417', '']\n",
      "['0 0.204086 0.490063 0.043797 0.054125', '0 0.945539 0.552740 0.037391 0.039896', '']\n",
      "['0 0.215305 0.506437 0.042734 0.052708', '0 0.918828 0.576958 0.033125 0.037042', '']\n",
      "['0 0.252164 0.558438 0.054484 0.051292', '0 0.920430 0.608292 0.055547 0.045583', '']\n",
      "['0 0.269258 0.573396 0.043797 0.047000', '0 0.932719 0.599031 0.043813 0.041312', '']\n",
      "['']\n",
      "['0 0.312633 0.568552 0.047016 0.041312', '0 0.945539 0.505740 0.041672 0.042729', '']\n",
      "['0 0.337633 0.542063 0.052359 0.047000', '0 0.946609 0.490062 0.039531 0.045583', '']\n",
      "['0 0.349922 0.532083 0.047000 0.047000', '0 0.945008 0.490781 0.040609 0.041313', '']\n",
      "['0 0.363914 0.527240 0.047016 0.041313', '0 0.937531 0.494344 0.034187 0.042729', '']\n",
      "['0 0.388914 0.536354 0.048078 0.052708', '0 0.934859 0.492917 0.039531 0.039875', '']\n",
      "['0 0.391055 0.544906 0.037391 0.044146', '0 0.944477 0.516427 0.037391 0.041313', '']\n",
      "['0 0.389984 0.539917 0.052344 0.054125', '0 0.952484 0.543490 0.049156 0.055562', '']\n",
      "['0 0.393187 0.545615 0.056625 0.065521', '0 0.960500 0.554167 0.045937 0.045583', '']\n",
      "['0 0.390523 0.550604 0.049141 0.049875', '0 0.959430 0.548469 0.035266 0.042729', '']\n",
      "['0 0.419359 0.574823 0.051281 0.049854', '0 0.959430 0.542771 0.037391 0.042750', '']\n",
      "['0 0.459430 0.598333 0.048078 0.059833', '0 0.958359 0.535646 0.039531 0.039875', '']\n",
      "['0 0.478664 0.603313 0.048078 0.052708', '0 0.956227 0.534927 0.035266 0.041313', '']\n",
      "['0 0.521930 0.591917 0.055547 0.052708', '0 0.958898 0.478667 0.047016 0.051292', '']\n",
      "['0 0.520859 0.619687 0.051281 0.059833', '0 0.957820 0.472260 0.042734 0.047021', '']\n",
      "['0 0.521398 0.697333 0.043797 0.069792', '0 0.939664 0.507875 0.038453 0.047000', '']\n",
      "['0 0.539555 0.692344 0.048078 0.045604', '0 0.900672 0.483656 0.039531 0.041312', '']\n",
      "['0 0.606867 0.574115 0.058766 0.062688', '0 0.880367 0.484365 0.030984 0.034187', '']\n",
      "['0 0.653336 0.568417 0.055547 0.054125', '0 0.845109 0.507865 0.035250 0.041312', '']\n",
      "['0 0.664555 0.571260 0.043797 0.056979', '0 0.825984 0.525823 0.035250 0.041313', '']\n",
      "['0 0.678445 0.649615 0.050203 0.056979', '']\n",
      "['0 0.717977 0.718698 0.050203 0.064104', '']\n",
      "['0 0.760711 0.759302 0.056609 0.065521', '']\n",
      "['0 0.806117 0.775677 0.053422 0.066937', '']\n",
      "['0 0.756437 0.400323 0.030969 0.034188', '0 0.795430 0.695198 0.047016 0.056979', '']\n",
      "['0 0.790625 0.745760 0.060906 0.078354', '']\n",
      "['0 0.794477 0.775813 0.049141 0.068375', '']\n",
      "['0 0.841906 0.802750 0.039531 0.061250', '']\n",
      "['0 0.725453 0.396052 0.033125 0.039896', '0 0.897461 0.720125 0.033109 0.061250', '']\n",
      "['0 0.722789 0.383938 0.036328 0.038458', '0 0.884641 0.639646 0.043813 0.068375', '']\n",
      "['0 0.867547 0.651740 0.052344 0.055563', '0 0.717977 0.383219 0.030984 0.048438', '']\n",
      "['0 0.857930 0.692344 0.041672 0.071229', '']\n",
      "['0 0.854195 0.789917 0.049141 0.052708', '']\n",
      "['']\n",
      "['0 0.830797 0.743062 0.047000 0.076917', '']\n",
      "['0 0.839234 0.727240 0.038469 0.064104', '']\n",
      "['0 0.865945 0.652458 0.032047 0.051292', '']\n",
      "['0 0.861141 0.654594 0.028844 0.055563', '']\n",
      "['0 0.902266 0.676677 0.034187 0.071229', '']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['0 0.955266 0.755875 0.050219 0.068375', '']\n",
      "['0 0.966906 0.795615 0.052344 0.064104', '']\n",
      "['0 0.964773 0.847615 0.060891 0.094021', '']\n",
      "['0 0.901203 0.890354 0.051281 0.091167', '']\n",
      "['0 0.969297 0.831948 0.061406 0.085479', '']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['0 0.908250 0.478104 0.035250 0.051292', '']\n",
      "['0 0.955156 0.510719 0.041656 0.035604', '']\n",
      "['0 0.960500 0.552031 0.039531 0.041312', '']\n",
      "['0 0.978656 0.594062 0.033125 0.045583', '']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['0 0.857930 0.971542 0.041672 0.051292', '']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['0 0.851523 0.942344 0.063047 0.101146', '']\n",
      "['0 0.879836 0.935937 0.059828 0.108250', '']\n",
      "['0 0.901203 0.935229 0.064094 0.106833', '']\n",
      "['0 0.873961 0.941635 0.067297 0.096854', '']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "frame-data/1/ frame000000.png\n",
      "frame-data/1/ frame000001.png\n",
      "frame-data/1/ frame000002.png\n",
      "frame-data/1/ frame000003.png\n",
      "frame-data/1/ frame000004.png\n",
      "frame-data/1/ frame000005.png\n",
      "frame-data/1/ frame000006.png\n",
      "frame-data/1/ frame000007.png\n",
      "frame-data/1/ frame000008.png\n",
      "frame-data/1/ frame000009.png\n",
      "frame-data/1/ frame000010.png\n",
      "frame-data/1/ frame000011.png\n",
      "frame-data/1/ frame000012.png\n",
      "frame-data/1/ frame000013.png\n",
      "frame-data/1/ frame000014.png\n",
      "frame-data/1/ frame000015.png\n",
      "frame-data/1/ frame000016.png\n",
      "frame-data/1/ frame000017.png\n",
      "frame-data/1/ frame000018.png\n",
      "frame-data/1/ frame000019.png\n",
      "frame-data/1/ frame000020.png\n",
      "frame-data/1/ frame000021.png\n",
      "frame-data/1/ frame000022.png\n",
      "frame-data/1/ frame000023.png\n",
      "frame-data/1/ frame000024.png\n",
      "frame-data/1/ frame000025.png\n",
      "frame-data/1/ frame000026.png\n",
      "frame-data/1/ frame000027.png\n",
      "frame-data/1/ frame000028.png\n",
      "frame-data/1/ frame000029.png\n",
      "frame-data/1/ frame000030.png\n",
      "frame-data/1/ frame000031.png\n",
      "frame-data/1/ frame000032.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(video_dir, img)\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m             images_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     images_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#example_img = cv2.imread('cctv-data/train/images/gmd_1_frame6_jpg.rf.8856761f366d26ec58bffe9f89f70414.jpg')\n",
    "#w, h, c = example_img.shape\n",
    "#\n",
    "#example_bbox = open(file='cctv-data/train/labels/gmd_1_frame6_jpg.rf.8856761f366d26ec58bffe9f89f70414.txt', mode=\"r\").read().split(' ')\n",
    "video_directories = ['frame-data/0/', 'frame-data/1/', 'frame-data/4/']\n",
    "\n",
    "# Prepare the data\n",
    "data = {'video_name': [], 'images': [], 'labels': []}\n",
    "\n",
    "# Iterate through each video directory\n",
    "for video_dir in video_directories:\n",
    "    images_path = os.path.join(video_dir, 'images/')\n",
    "    labels_path = os.path.join(video_dir, 'labels/')\n",
    "    \n",
    "    # Get list of images\n",
    "    if os.path.exists(images_path):\n",
    "        images_list = []\n",
    "        for img in sorted(os.listdir(images_path)):\n",
    "            print(video_dir, img)\n",
    "            if img.endswith('.png'):\n",
    "                images_list.append(cv2.imread(os.path.join(images_path, img))/255)\n",
    "    else:\n",
    "        images_list = []\n",
    "    \n",
    "    # Get list of labels\n",
    "    if os.path.exists(labels_path):\n",
    "        labels_list = []\n",
    "        for label in sorted(os.listdir(labels_path)):\n",
    "            if label.endswith('.txt'):\n",
    "                label = open(os.path.join(labels_path, label))\n",
    "                lines = label.read().split('\\n')\n",
    "                print(lines)\n",
    "                bboxes = []\n",
    "                for line in lines:\n",
    "                    bbox = line.split(' ')\n",
    "                    bboxes.append(bbox)\n",
    "                labels_list.append(bboxes)\n",
    "                label.close()\n",
    "    else:\n",
    "        labels_list = []\n",
    "    \n",
    "    data['video_name'].append(video_dir.strip('/'))\n",
    "    data['images'].append(images_list)\n",
    "    data['labels'].append(labels_list)\n",
    "\n",
    "# Create the dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2, 'labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def make_anchors(x, strides, offset=0.5):\n",
    "    \"\"\"\n",
    "    Generate anchors from features\n",
    "    \"\"\"\n",
    "    assert x is not None\n",
    "    anchor_points, stride_tensor = [], []\n",
    "    for i, stride in enumerate(strides):\n",
    "        _, _, h, w = x[i].shape\n",
    "        sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset  # shift x\n",
    "        sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset  # shift y\n",
    "        sy, sx = torch.meshgrid(sy, sx)\n",
    "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n",
    "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
    "\n",
    "\n",
    "\n",
    "def pad(k, p=None, d=1):\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1\n",
    "    if p is None:\n",
    "        p = k // 2\n",
    "    return p\n",
    "\n",
    "\n",
    "def fuse_conv(conv, norm):\n",
    "    fused_conv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                 conv.out_channels,\n",
    "                                 kernel_size=conv.kernel_size,\n",
    "                                 stride=conv.stride,\n",
    "                                 padding=conv.padding,\n",
    "                                 groups=conv.groups,\n",
    "                                 bias=True).requires_grad_(False).to(conv.weight.device)\n",
    "\n",
    "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.eps + norm.running_var)))\n",
    "    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n",
    "\n",
    "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
    "    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n",
    "    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n",
    "\n",
    "    return fused_conv\n",
    "\n",
    "\n",
    "class Conv(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=1, s=1, p=None, d=1, g=1):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch, out_ch, k, s, pad(k, p, d), d, g, False)\n",
    "        self.norm = torch.nn.BatchNorm2d(out_ch, 0.001, 0.03)\n",
    "        self.relu = torch.nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.norm(self.conv(x)))\n",
    "\n",
    "    def fuse_forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, ch, add=True):\n",
    "        super().__init__()\n",
    "        self.add_m = add\n",
    "        self.res_m = torch.nn.Sequential(Conv(ch, ch, 3),\n",
    "                                         Conv(ch, ch, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.res_m(x) + x if self.add_m else self.res_m(x)\n",
    "\n",
    "\n",
    "class CSP(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, n=1, add=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, out_ch // 2)\n",
    "        self.conv2 = Conv(in_ch, out_ch // 2)\n",
    "        self.conv3 = Conv((2 + n) * out_ch // 2, out_ch)\n",
    "        self.res_m = torch.nn.ModuleList(Residual(out_ch // 2, add) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = [self.conv1(x), self.conv2(x)]\n",
    "        y.extend(m(y[-1]) for m in self.res_m)\n",
    "        return self.conv3(torch.cat(y, dim=1))\n",
    "\n",
    "\n",
    "class SPP(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_ch, in_ch // 2)\n",
    "        self.conv2 = Conv(in_ch * 2, out_ch)\n",
    "        self.res_m = torch.nn.MaxPool2d(k, 1, k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y1 = self.res_m(x)\n",
    "        y2 = self.res_m(y1)\n",
    "        return self.conv2(torch.cat([x, y1, y2, self.res_m(y2)], 1))\n",
    "\n",
    "\n",
    "class DarkNet(torch.nn.Module):\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        p1 = [Conv(width[0], width[1], 3, 2)]\n",
    "        p2 = [Conv(width[1], width[2], 3, 2),\n",
    "              CSP(width[2], width[2], depth[0])]\n",
    "        p3 = [Conv(width[2], width[3], 3, 2),\n",
    "              CSP(width[3], width[3], depth[1])]\n",
    "        p4 = [Conv(width[3], width[4], 3, 2),\n",
    "              CSP(width[4], width[4], depth[2])]\n",
    "        p5 = [Conv(width[4], width[5], 3, 2),\n",
    "              CSP(width[5], width[5], depth[0]),\n",
    "              SPP(width[5], width[5])]\n",
    "\n",
    "        self.p1 = torch.nn.Sequential(*p1)\n",
    "        self.p2 = torch.nn.Sequential(*p2)\n",
    "        self.p3 = torch.nn.Sequential(*p3)\n",
    "        self.p4 = torch.nn.Sequential(*p4)\n",
    "        self.p5 = torch.nn.Sequential(*p5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1(x)\n",
    "        p2 = self.p2(p1)\n",
    "        p3 = self.p3(p2)\n",
    "        p4 = self.p4(p3)\n",
    "        p5 = self.p5(p4)\n",
    "        return p3, p4, p5\n",
    "\n",
    "\n",
    "class DarkFPN(torch.nn.Module):\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        self.up = torch.nn.Upsample(None, 2)\n",
    "        self.h1 = CSP(width[4] + width[5], width[4], depth[0], False)\n",
    "        self.h2 = CSP(width[3] + width[4], width[3], depth[0], False)\n",
    "        self.h3 = Conv(width[3], width[3], 3, 2)\n",
    "        self.h4 = CSP(width[3] + width[4], width[4], depth[0], False)\n",
    "        self.h5 = Conv(width[4], width[4], 3, 2)\n",
    "        self.h6 = CSP(width[4] + width[5], width[5], depth[0], False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p3, p4, p5 = x\n",
    "        h1 = self.h1(torch.cat([self.up(p5), p4], 1))\n",
    "        h2 = self.h2(torch.cat([self.up(h1), p3], 1))\n",
    "        h4 = self.h4(torch.cat([self.h3(h2), h1], 1))\n",
    "        h6 = self.h6(torch.cat([self.h5(h4), p5], 1))\n",
    "        return h2, h4, h6\n",
    "\n",
    "\n",
    "class DFL(torch.nn.Module):\n",
    "    # Integral module of Distribution Focal Loss (DFL)\n",
    "    # Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "    def __init__(self, ch=16):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.conv = torch.nn.Conv2d(ch, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(ch, dtype=torch.float).view(1, ch, 1, 1)\n",
    "        self.conv.weight.data[:] = torch.nn.Parameter(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, a = x.shape\n",
    "        x = x.view(b, 4, self.ch, a).transpose(2, 1)\n",
    "        return self.conv(x.softmax(1)).view(b, 4, a)\n",
    "\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "    anchors = torch.empty(0)\n",
    "    strides = torch.empty(0)\n",
    "\n",
    "    def __init__(self, nc=80, filters=()):\n",
    "        super().__init__()\n",
    "        self.ch = 16  # DFL channels\n",
    "        self.nc = nc  # number of classes\n",
    "        self.nl = len(filters)  # number of detection layers\n",
    "        self.no = nc + self.ch * 4  # number of outputs per anchor\n",
    "        self.stride = torch.zeros(self.nl)  # strides computed during build\n",
    "\n",
    "        c1 = max(filters[0], self.nc)\n",
    "        c2 = max((filters[0] // 4, self.ch * 4))\n",
    "\n",
    "        self.dfl = DFL(self.ch)\n",
    "        self.cls = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c1, 3),\n",
    "                                                           Conv(c1, c1, 3),\n",
    "                                                           torch.nn.Conv2d(c1, self.nc, 1)) for x in filters)\n",
    "        self.box = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c2, 3),\n",
    "                                                           Conv(c2, c2, 3),\n",
    "                                                           torch.nn.Conv2d(c2, 4 * self.ch, 1)) for x in filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.nl):\n",
    "            x[i] = torch.cat((self.box[i](x[i]), self.cls[i](x[i])), 1)\n",
    "        if self.training:\n",
    "            return x\n",
    "        self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n",
    "\n",
    "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
    "        box, cls = x.split((self.ch * 4, self.nc), 1)\n",
    "        a, b = torch.split(self.dfl(box), 2, 1)\n",
    "        a = self.anchors.unsqueeze(0) - a\n",
    "        b = self.anchors.unsqueeze(0) + b\n",
    "        box = torch.cat(((a + b) / 2, b - a), 1)\n",
    "        return torch.cat((box * self.strides, cls.sigmoid()), 1)\n",
    "\n",
    "    def initialize_biases(self):\n",
    "        # Initialize biases\n",
    "        # WARNING: requires stride availability\n",
    "        m = self\n",
    "        for a, b, s in zip(m.box, m.cls, m.stride):\n",
    "            a[-1].bias.data[:] = 1.0  # box\n",
    "            # cls (.01 objects, 80 classes, 640 img)\n",
    "            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n",
    "\n",
    "\n",
    "class YOLO(torch.nn.Module):\n",
    "    def __init__(self, width, depth, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = DarkNet(width, depth)\n",
    "        self.fpn = DarkFPN(width, depth)\n",
    "\n",
    "        img_dummy = torch.zeros(1, 3, 256, 256)\n",
    "        self.head = Head(num_classes, (width[3], width[4], width[5]))\n",
    "        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n",
    "        self.stride = self.head.stride\n",
    "        self.head.initialize_biases()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        P3, P4, P5 = self.fpn(x)\n",
    "            \n",
    "        P3_h, P4_h, P5_h = self.head(list(x))\n",
    "        return P3_h, P4_h, P5_h\n",
    "\n",
    "    def fuse(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) is Conv and hasattr(m, 'norm'):\n",
    "                m.conv = fuse_conv(m.conv, m.norm)\n",
    "                m.forward = m.fuse_forward\n",
    "                delattr(m, 'norm')\n",
    "        return self\n",
    "\n",
    "def yolo_v8_n(num_classes: int = 80):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 16, 32, 64, 128, 256]\n",
    "    return YOLO(width, depth, num_classes)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def human_readable(num, dec=2):\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return ('{:.' + str(dec) + 'f}{}').format(num, ['', 'K', 'M', 'B', 'T'][magnitude])\n",
    "\n",
    "md = yolo_v8_n(num_classes=1)\n",
    "human_readable(count_parameters(md))\n",
    "expl = torch.randn(1, 3, 640, 480)\n",
    "out = md(expl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## and yolomamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835.84K\n",
      "Input shape: torch.Size([4, 256, 20, 15])\n",
      "Output shape: torch.Size([4, 256, 20, 15])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nna\n",
    "import torch.nn.functional as F\n",
    "from scripts.mamba_block import MambaConfig, Mamba\n",
    "\n",
    "class FeatureMap_Bottleneck(nn.Module):\n",
    "    def __init__(self, embedding_size, n_layers_mamba=1, input_channels = 256, reduction = None):\n",
    "        super(FeatureMap_Bottleneck, self).__init__()\n",
    "        if reduction == None: reduction = embedding_size\n",
    "        self.conv1 = nn.Conv2d(input_channels, reduction, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Downscale by 2\n",
    "        self.conv2 = nn.Conv2d(reduction, embedding_size, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # Downscale by 2\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(embedding_size, embedding_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(2*embedding_size, embedding_size, kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(2*embedding_size, embedding_size, kernel_size=3, padding=1)  # Adjusted input channels\n",
    "        self.upconv1 = nn.ConvTranspose2d(embedding_size, reduction, kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(2*reduction, input_channels, kernel_size=3, padding=1)  # Adjusted input channels\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        config = MambaConfig(d_model=embedding_size, n_layers=n_layers_mamba)\n",
    "        self.mamba = Mamba(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downscale\n",
    "        conv1_out = self.conv1(x)\n",
    "        x = self.pool1(conv1_out)\n",
    "        conv2_out = self.conv2(x)\n",
    "        x = self.pool2(conv2_out)\n",
    "        \n",
    "        bottleneck = self.conv3(x)\n",
    "        flat_bottleneck = F.adaptive_avg_pool2d(bottleneck, (1, 1))  # Shape: (S, C, 1, 1)\n",
    "        frame_embedding = flat_bottleneck.reshape(S, self.embedding_size).unsqueeze(0)\n",
    "        \n",
    "        frame_embedding = self.mamba(frame_embedding)\n",
    "\n",
    "        frame_embedding = frame_embedding.squeeze().unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        x = self.crop_and_concat(frame_embedding, bottleneck)\n",
    "        x = self.upconv2(x)\n",
    "        x = self.crop_and_concat(x, conv2_out)\n",
    "        x = self.conv4(x)\n",
    "        x = self.upconv1(x)\n",
    "        x = self.crop_and_concat(x, conv1_out)\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        \"\"\"\n",
    "        Crop the downsampled feature map to match the size of the upsampled feature map and concatenate them.\n",
    "        \"\"\"\n",
    "        _, _, h, w = bypass.size()\n",
    "        upsampled = F.interpolate(upsampled, size=(h, w), mode='bilinear', align_corners=True)\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "S = 4  # Batch size\n",
    "input_tensor = torch.randn(S, 256, 20, 15)\n",
    "\n",
    "bottleneck = FeatureMap_Bottleneck(embedding_size=64, n_layers_mamba=6)\n",
    "print(human_readable(count_parameters(bottleneck)))\n",
    "output_tensor = bottleneck(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(\"Output shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.85M\n"
     ]
    }
   ],
   "source": [
    "class YOLO_Mamba(torch.nn.Module):\n",
    "    def __init__(self, width, depth, num_classes, n_layers_mamba, embedding_size=64):\n",
    "        super().__init__()\n",
    "        self.net = DarkNet(width, depth)\n",
    "        self.fpn = DarkFPN(width, depth)\n",
    "\n",
    "        self.featuremap_bottleneck = FeatureMap_Bottleneck(input_channels=width[-1], embedding_size=embedding_size,\n",
    "                                                            n_layers_mamba=n_layers_mamba)\n",
    "        img_dummy = torch.zeros(1, 3, 256, 256)\n",
    "        self.head = Head(num_classes, (width[3], width[4], width[5]))\n",
    "        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n",
    "        self.stride = self.head.stride\n",
    "        self.head.initialize_biases()\n",
    "\n",
    "    def forward(self, x, sequential=False):\n",
    "        x = self.net(x)\n",
    "        P3, P4, P5 = self.fpn(x)\n",
    "        \n",
    "        if sequential:\n",
    "            P5 = self.featuremap_bottleneck(P5)\n",
    "\n",
    "        out = self.head(list(x))\n",
    "        return out\n",
    "\n",
    "    def fuse(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) is Conv and hasattr(m, 'norm'):\n",
    "                m.conv = fuse_conv(m.conv, m.norm)\n",
    "                m.forward = m.fuse_forward\n",
    "                delattr(m, 'norm')\n",
    "        return self\n",
    "\n",
    "def yolo_mamba_nano(num_classes: int = 80, n_layers_mamba=1, embedding_size=64):\n",
    "    depth = [1, 2, 2]\n",
    "    width = [3, 16, 32, 64, 128, 256]\n",
    "    return YOLO_Mamba(width, depth, num_classes=num_classes, n_layers_mamba=n_layers_mamba, embedding_size=embedding_size)\n",
    "\n",
    "wow = yolo_mamba_nano(num_classes=1, n_layers_mamba=6)\n",
    "print(human_readable(count_parameters(wow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 3, 640, 640])\n",
      "Output shape: 4 torch.Size([5, 8400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxine/miniconda3/envs/louis/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403388920/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(S, 3, 640, 640)\n",
    "\n",
    "wow.head.training = False\n",
    "output_tensor = wow(input_tensor)\n",
    "\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "\n",
    "print(\"Output shape:\", len(output_tensor),  output_tensor[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gmd_1_frame74_jpg.rf.97e64219df6d595ade4015d8b70b265f.jpg',\n",
       " 'mock_attack_frame562_jpg.rf.45c430c44d9d58d134a3490c80a63795.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.listdir('weapon-dataset/images/train/')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n",
      "/home/maxine/miniconda3/envs/louis/lib/python3.10/site-packages/albumentations/core/composition.py:151: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  warnings.warn(f\"Got processor for {proc.default_data_name}, but no transform to process it.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " torch.Size([3, 640, 640]),\n",
       " tensor([[0.0000, 0.0000, 0.5203, 0.4938, 0.0219, 0.0586]]),\n",
       " ((640, 640), ((1.0, 1.0), (0.0, 0.0))))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "\n",
    "FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, filedir, input_size, params, augment):\n",
    "\n",
    "        self.params = params\n",
    "        self.mosaic = augment\n",
    "        self.augment = augment\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Read labels\n",
    "        filenames = [filedir + fname for fname in os.listdir(filedir) if fname.endswith(FORMATS)]\n",
    "        cache = self.load_label(filenames)\n",
    "        labels, shapes = zip(*cache.values())\n",
    "        self.labels = list(labels)\n",
    "        self.shapes = numpy.array(shapes, dtype=numpy.float64)\n",
    "        self.filenames = list(cache.keys())  # update\n",
    "        self.n = len(shapes)  # number of samples\n",
    "        self.indices = range(self.n)\n",
    "        # Albumentations (optional, only used if package is installed)\n",
    "        self.albumentations = Albumentations()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]\n",
    "\n",
    "        params = self.params\n",
    "        mosaic = self.mosaic and random.random() < params['mosaic']\n",
    "\n",
    "        if mosaic:\n",
    "            shapes = None\n",
    "            # Load MOSAIC\n",
    "            image, label = self.load_mosaic(index, params)\n",
    "            # MixUp augmentation\n",
    "            if random.random() < params['mix_up']:\n",
    "                index = random.choice(self.indices)\n",
    "                mix_image1, mix_label1 = image, label\n",
    "                mix_image2, mix_label2 = self.load_mosaic(index, params)\n",
    "\n",
    "                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n",
    "        else:\n",
    "            # Load image\n",
    "            image, shape = self.load_image(index)\n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            # Resize\n",
    "            image, ratio, pad = resize(image, self.input_size, self.augment)\n",
    "            shapes = shape, ((h / shape[0], w / shape[1]), pad) \n",
    "\n",
    "            label = self.labels[index].copy()\n",
    "            if label.size:\n",
    "                label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n",
    "            if self.augment:\n",
    "                image, label = random_perspective(image, label, params)\n",
    "        nl = len(label)  # number of labels\n",
    "        if nl:\n",
    "            label[:, 1:5] = xy2wh(label[:, 1:5], image.shape[1], image.shape[0])\n",
    "\n",
    "        if self.augment:\n",
    "            # Albumentations\n",
    "            image, label = self.albumentations(image, label)\n",
    "            nl = len(label)  # update after albumentations\n",
    "            # HSV color-space\n",
    "            augment_hsv(image, params)\n",
    "            # Flip up-down\n",
    "            if random.random() < params['flip_ud']:\n",
    "                image = numpy.flipud(image)\n",
    "                if nl:\n",
    "                    label[:, 2] = 1 - label[:, 2]\n",
    "            # Flip left-right\n",
    "            if random.random() < params['flip_lr']:\n",
    "                image = numpy.fliplr(image)\n",
    "                if nl:\n",
    "                    label[:, 1] = 1 - label[:, 1]\n",
    "\n",
    "        target = torch.zeros((nl, 6))\n",
    "        if nl:\n",
    "            target[:, 1:] = torch.from_numpy(label)\n",
    "\n",
    "        # Convert HWC to CHW, BGR to RGB\n",
    "        sample = image.transpose((2, 0, 1))[::-1]\n",
    "        sample = numpy.ascontiguousarray(sample)\n",
    "\n",
    "        return torch.from_numpy(sample), target, shapes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        image = cv2.imread(self.filenames[i])\n",
    "        h, w = image.shape[:2]\n",
    "        r = self.input_size / max(h, w)\n",
    "        if r != 1:\n",
    "            image = cv2.resize(image,\n",
    "                               dsize=(int(w * r), int(h * r)),\n",
    "                               interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n",
    "        return image, (h, w)\n",
    "\n",
    "    def load_mosaic(self, index, params):\n",
    "        label4 = []\n",
    "        image4 = numpy.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=numpy.uint8)\n",
    "        y1a, y2a, x1a, x2a, y1b, y2b, x1b, x2b = (None, None, None, None, None, None, None, None)\n",
    "\n",
    "        border = [-self.input_size // 2, -self.input_size // 2]\n",
    "\n",
    "        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "\n",
    "        indices = [index] + random.choices(self.indices, k=3)\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        for i, index in enumerate(indices):\n",
    "            # Load image\n",
    "            image, _ = self.load_image(index)\n",
    "            shape = image.shape\n",
    "            if i == 0:  # top left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = xc\n",
    "                y2a = yc\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = shape[1]\n",
    "                y2b = shape[0]\n",
    "            if i == 1:  # top right\n",
    "                x1a = xc\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = yc\n",
    "                x1b = 0\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = shape[0]\n",
    "            if i == 2:  # bottom left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = yc\n",
    "                x2a = xc\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = 0\n",
    "                x2b = shape[1]\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "            if i == 3:  # bottom right\n",
    "                x1a = xc\n",
    "                y1a = yc\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = 0\n",
    "                y1b = 0\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "\n",
    "            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "            pad_w = x1a - x1b\n",
    "            pad_h = y1a - y1b\n",
    "\n",
    "            # Labels\n",
    "            label = self.labels[index].copy()\n",
    "            if len(label):\n",
    "                label[:, 1:] = wh2xy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n",
    "            label4.append(label)\n",
    "\n",
    "        # Concat/clip labels\n",
    "        label4 = numpy.concatenate(label4, 0)\n",
    "        for x in label4[:, 1:]:\n",
    "            numpy.clip(x, 0, 2 * self.input_size, out=x)\n",
    "\n",
    "        # Augment\n",
    "        image4, label4 = random_perspective(image4, label4, params, border)\n",
    "\n",
    "        return image4, label4\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        samples, targets, shapes = zip(*batch)\n",
    "        for i, item in enumerate(targets):\n",
    "            item[:, 0] = i  # add target image index\n",
    "        return torch.stack(samples, 0), torch.cat(targets, 0), shapes\n",
    "\n",
    "    @staticmethod\n",
    "    def load_label(filenames):\n",
    "        path = f'{os.path.dirname(filenames[0])}.cache'\n",
    "        if os.path.exists(path):\n",
    "            return torch.load(path)\n",
    "        x = {}\n",
    "        for filename in filenames:\n",
    "                # verify images\n",
    "                with open(filename, 'rb') as f:\n",
    "                    image = Image.open(f)\n",
    "                    image.verify()  # PIL verify\n",
    "                shape = image.size  # image size\n",
    "                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n",
    "                assert image.format.lower() in FORMATS, f'invalid image format {image.format}'\n",
    "\n",
    "                # verify labels\n",
    "                a = f'{os.sep}images{os.sep}'\n",
    "                b = f'{os.sep}labels{os.sep}'\n",
    "                if os.path.isfile(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'):\n",
    "                    with open(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt') as f:\n",
    "                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
    "                        label = numpy.array(label, dtype=numpy.float32)\n",
    "                    nl = len(label)\n",
    "                    if nl:\n",
    "                        assert label.shape[1] == 5, 'labels require 5 columns'\n",
    "                        assert (label >= 0).all(), 'negative label values'\n",
    "                        assert (label[:, 1:] <= 1).all(), 'non-normalized coordinates'\n",
    "                        _, i = numpy.unique(label, axis=0, return_index=True)\n",
    "                        if len(i) < nl:  # duplicate row check\n",
    "                            label = label[i]  # remove duplicates\n",
    "                    else:\n",
    "                        label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
    "                else:\n",
    "                    label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
    "                if filename:\n",
    "                    x[filename] = [label, shape]\n",
    "        torch.save(x, path)\n",
    "        return x\n",
    "\n",
    "def wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n",
    "    # Convert nx4 boxes\n",
    "    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = numpy.copy(x)\n",
    "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n",
    "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n",
    "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n",
    "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def xy2wh(x, w=640, h=640):\n",
    "    # warning: inplace clip\n",
    "    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n",
    "    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n",
    "\n",
    "    # Convert nx4 boxes\n",
    "    # from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    y = numpy.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def resample():\n",
    "    choices = (cv2.INTER_AREA,\n",
    "               cv2.INTER_CUBIC,\n",
    "               cv2.INTER_LINEAR,\n",
    "               cv2.INTER_NEAREST,\n",
    "               cv2.INTER_LANCZOS4)\n",
    "    return random.choice(seq=choices)\n",
    "\n",
    "\n",
    "def augment_hsv(image, params):\n",
    "    # HSV color-space augmentation\n",
    "    h = params['hsv_h']\n",
    "    s = params['hsv_s']\n",
    "    v = params['hsv_v']\n",
    "\n",
    "    r = numpy.random.uniform(-1, 1, 3) * [h, s, v] + 1\n",
    "    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n",
    "\n",
    "    x = numpy.arange(0, 256, dtype=r.dtype)\n",
    "    lut_h = ((x * r[0]) % 180).astype('uint8')\n",
    "    lut_s = numpy.clip(x * r[1], 0, 255).astype('uint8')\n",
    "    lut_v = numpy.clip(x * r[2], 0, 255).astype('uint8')\n",
    "\n",
    "    im_hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n",
    "    cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=image)  # no return needed\n",
    "\n",
    "\n",
    "def resize(image, input_size, augment):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = image.shape[:2]  # current shape [height, width]\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(input_size / shape[0], input_size / shape[1])\n",
    "    if not augment:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    w = (input_size - pad[0]) / 2\n",
    "    h = (input_size - pad[1]) / 2\n",
    "\n",
    "    if shape[::-1] != pad:  # resize\n",
    "        image = cv2.resize(image,\n",
    "                           dsize=pad,\n",
    "                           interpolation=resample() if augment else cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n",
    "    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)  # add border\n",
    "    return image, (r, r), (w, h)\n",
    "\n",
    "\n",
    "def candidates(box1, box2):\n",
    "    # box1(4,n), box2(4,n)\n",
    "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
    "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
    "    aspect_ratio = numpy.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n",
    "    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n",
    "\n",
    "\n",
    "def random_perspective(samples, targets, params, border=(0, 0)):\n",
    "    h = samples.shape[0] + border[0] * 2\n",
    "    w = samples.shape[1] + border[1] * 2\n",
    "\n",
    "    # Center\n",
    "    center = numpy.eye(3)\n",
    "    center[0, 2] = -samples.shape[1] / 2  # x translation (pixels)\n",
    "    center[1, 2] = -samples.shape[0] / 2  # y translation (pixels)\n",
    "\n",
    "    # Perspective\n",
    "    perspective = numpy.eye(3)\n",
    "\n",
    "    # Rotation and Scale\n",
    "    rotate = numpy.eye(3)\n",
    "    a = random.uniform(-params['degrees'], params['degrees'])\n",
    "    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n",
    "    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
    "\n",
    "    # Shear\n",
    "    shear = numpy.eye(3)\n",
    "    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "\n",
    "    # Translation\n",
    "    translate = numpy.eye(3)\n",
    "    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n",
    "    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n",
    "\n",
    "    # Combined rotation matrix, order of operations (right to left) is IMPORTANT\n",
    "    matrix = translate @ shear @ rotate @ perspective @ center\n",
    "    if (border[0] != 0) or (border[1] != 0) or (matrix != numpy.eye(3)).any():  # image changed\n",
    "        samples = cv2.warpAffine(samples, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n",
    "\n",
    "    # Transform label coordinates\n",
    "    n = len(targets)\n",
    "    if n:\n",
    "        xy = numpy.ones((n * 4, 3))\n",
    "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "        xy = xy @ matrix.T  # transform\n",
    "        xy = xy[:, :2].reshape(n, 8)  # perspective rescale or affine\n",
    "\n",
    "        # create new boxes\n",
    "        x = xy[:, [0, 2, 4, 6]]\n",
    "        y = xy[:, [1, 3, 5, 7]]\n",
    "        new = numpy.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
    "\n",
    "        # clip\n",
    "        new[:, [0, 2]] = new[:, [0, 2]].clip(0, w)\n",
    "        new[:, [1, 3]] = new[:, [1, 3]].clip(0, h)\n",
    "\n",
    "        # filter candidates\n",
    "        indices = candidates(box1=targets[:, 1:5].T * s, box2=new.T)\n",
    "        targets = targets[indices]\n",
    "        targets[:, 1:5] = new[indices]\n",
    "\n",
    "    return samples, targets\n",
    "\n",
    "\n",
    "def mix_up(image1, label1, image2, label2):\n",
    "    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n",
    "    alpha = numpy.random.beta(32.0, 32.0)  # mix-up ratio, alpha=beta=32.0\n",
    "    image = (image1 * alpha + image2 * (1 - alpha)).astype(numpy.uint8)\n",
    "    label = numpy.concatenate((label1, label2), 0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "class Albumentations:\n",
    "    def __init__(self):\n",
    "        self.transform = None\n",
    "        try:\n",
    "            import albumentations as album\n",
    "\n",
    "            transforms = [album.Blur(p=0.01),\n",
    "                          album.CLAHE(p=0.01),\n",
    "                          album.ToGray(p=0.01),\n",
    "                          album.MedianBlur(p=0.01)]\n",
    "            self.transform = album.Compose(transforms,\n",
    "                                           album.BboxParams('yolo', ['class_labels']))\n",
    "\n",
    "        except ImportError:  # package not installed, skip\n",
    "            pass\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if self.transform:\n",
    "            x = self.transform(image=image,\n",
    "                               bboxes=label[:, 1:],\n",
    "                               class_labels=label[:, 0])\n",
    "            image = x['image']\n",
    "            label = numpy.array([[c, *b] for c, b in zip(x['class_labels'], x['bboxes'])])\n",
    "        return image, label\n",
    "import yaml\n",
    "with open(os.path.join('scripts', 'params.yaml'), errors='ignore') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "\n",
    "dat = Dataset(filedir='weapon-dataset/images/train/', input_size=640,  params=params, augment=False)\n",
    "it = dat.__getitem__(5)\n",
    "len(it), it[0].shape, it[1], it[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the rest of utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxine/miniconda3/envs/louis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn.functional import cross_entropy, one_hot\n",
    "\n",
    "\n",
    "def setup_seed():\n",
    "    \"\"\"\n",
    "    Setup random seed.\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    numpy.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def setup_multi_processes():\n",
    "    \"\"\"\n",
    "    Setup multi-processing environment variables.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    from os import environ\n",
    "    from platform import system\n",
    "\n",
    "    # set multiprocess start method as `fork` to speed up the training\n",
    "    if system() != 'Windows':\n",
    "        torch.multiprocessing.set_start_method('fork', force=True)\n",
    "\n",
    "    # disable opencv multithreading to avoid system being overloaded\n",
    "    cv2.setNumThreads(0)\n",
    "\n",
    "    # setup OMP threads\n",
    "    if 'OMP_NUM_THREADS' not in environ:\n",
    "        environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    # setup MKL threads\n",
    "    if 'MKL_NUM_THREADS' not in environ:\n",
    "        environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "def scale(coords, shape1, shape2, ratio_pad=None):\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(shape1[0] / shape2[0], shape1[1] / shape2[1])  # gain  = old / new\n",
    "        pad = (shape1[1] - shape2[1] * gain) / 2, (shape1[0] - shape2[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
    "    coords[:, :4] /= gain\n",
    "\n",
    "    coords[:, 0].clamp_(0, shape2[1])  # x1\n",
    "    coords[:, 1].clamp_(0, shape2[0])  # y1\n",
    "    coords[:, 2].clamp_(0, shape2[1])  # x2\n",
    "    coords[:, 3].clamp_(0, shape2[0])  # y2\n",
    "    return coords\n",
    "\n",
    "\n",
    "def make_anchors(x, strides, offset=0.5):\n",
    "    \"\"\"\n",
    "    Generate anchors from features\n",
    "    \"\"\"\n",
    "    assert x is not None\n",
    "    anchor_points, stride_tensor = [], []\n",
    "    for i, stride in enumerate(strides):\n",
    "        _, _, h, w = x[i].shape\n",
    "        sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset  # shift x\n",
    "        sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset  # shift y\n",
    "        sy, sx = torch.meshgrid(sy, sx)\n",
    "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n",
    "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
    "\n",
    "\n",
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    (a1, a2), (b1, b2) = box1[:, None].chunk(2, 2), box2.chunk(2, 1)\n",
    "    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "\n",
    "    # IoU = intersection / (area1 + area2 - intersection)\n",
    "    box1 = box1.T\n",
    "    box2 = box2.T\n",
    "\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    return intersection / (area1[:, None] + area2 - intersection)\n",
    "\n",
    "\n",
    "def wh2xy_ut(x):\n",
    "    y = x.clone()\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_threshold=0.25, iou_threshold=0.45):\n",
    "    nc = prediction.shape[1] - 4  # number of classes\n",
    "    xc = prediction[:, 4:4 + nc].amax(1) > conf_threshold  # candidates\n",
    "\n",
    "    # Settings\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_det = 300  # the maximum number of boxes to keep after NMS\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
    "    for index, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        x = x.transpose(0, -1)[xc[index]]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Detections matrix nx6 (box, conf, cls)\n",
    "        box, cls = x.split((4, nc), 1)\n",
    "        # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "        box = wh2xy_ut(box)\n",
    "        if nc > 1:\n",
    "            i, j = (cls > conf_threshold).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = cls.max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_threshold]\n",
    "        # Check shape\n",
    "        if not x.shape[0]:  # no boxes\n",
    "            continue\n",
    "        # sort by confidence and remove excess boxes\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "        outputs[index] = x[i]\n",
    "        if (time.time() - start) > 0.5 + 0.05 * prediction.shape[0]:\n",
    "            print(f'WARNING ⚠️ NMS time limit {0.5 + 0.05 * prediction.shape[0]:.3f}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def smooth(y, f=0.05):\n",
    "    # Box filter of fraction f\n",
    "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
    "    p = numpy.ones(nf // 2)  # ones padding\n",
    "    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
    "    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n",
    "\n",
    "\n",
    "def compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n",
    "    \"\"\"\n",
    "    Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:  True positives (nparray, nx1 or nx10).\n",
    "        conf:  Object-ness value from 0-1 (nparray).\n",
    "        pred_cls:  Predicted object classes (nparray).\n",
    "        target_cls:  True object classes (nparray).\n",
    "    # Returns\n",
    "        The average precision\n",
    "    \"\"\"\n",
    "    # Sort by object-ness\n",
    "    i = numpy.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = numpy.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    p = numpy.zeros((nc, 1000))\n",
    "    r = numpy.zeros((nc, 1000))\n",
    "    ap = numpy.zeros((nc, tp.shape[1]))\n",
    "    px, py = numpy.linspace(0, 1, 1000), []  # for plotting\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        nl = nt[ci]  # number of labels\n",
    "        no = i.sum()  # number of outputs\n",
    "        if no == 0 or nl == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (nl + eps)  # recall curve\n",
    "        # negative x, xp because xp decreases\n",
    "        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n",
    "            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n",
    "\n",
    "            # Compute the precision envelope\n",
    "            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n",
    "\n",
    "            # Integrate area under curve\n",
    "            x = numpy.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
    "            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + eps)\n",
    "\n",
    "    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    m_pre, m_rec = p.mean(), r.mean()\n",
    "    map50, mean_ap = ap50.mean(), ap.mean()\n",
    "    return tp, fp, m_pre, m_rec, map50, mean_ap\n",
    "\n",
    "\n",
    "def strip_optimizer(filename):\n",
    "    x = torch.load(filename, map_location=torch.device('cpu'))\n",
    "    x['model'].half()  # to FP16\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = False\n",
    "    torch.save(x, filename)\n",
    "\n",
    "\n",
    "def clip_gradients(model, max_norm=10.0):\n",
    "    parameters = model.parameters()\n",
    "    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"\n",
    "    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n",
    "    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n",
    "    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n",
    "        # Create EMA\n",
    "        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n",
    "        self.updates = updates  # number of EMA updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "\n",
    "            msd = model.state_dict()  # model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1 - d) * msd[k].detach()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        self.sum = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, v, n):\n",
    "        if not math.isnan(float(v)):\n",
    "            self.num = self.num + n\n",
    "            self.sum = self.sum + v * n\n",
    "            self.avg = self.sum / self.num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy, one_hot\n",
    "class ComputeLoss:\n",
    "    def __init__(self, model, params):\n",
    "        super().__init__()\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "\n",
    "        m = model.head  # Head() module\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.stride = m.stride  # model strides\n",
    "        self.nc = m.nc  # number of classes\n",
    "        self.no = m.no\n",
    "        self.device = device\n",
    "        self.params = params\n",
    "\n",
    "        # task aligned assigner\n",
    "        self.top_k = 10\n",
    "        self.alpha = 0.5\n",
    "        self.beta = 6.0\n",
    "        self.eps = 1e-9\n",
    "\n",
    "        self.bs = 1\n",
    "        self.num_max_boxes = 0\n",
    "        # DFL Loss params\n",
    "        self.dfl_ch = m.dfl.ch\n",
    "        self.project = torch.arange(self.dfl_ch, dtype=torch.float, device=device)\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        #x = outputs[1] if isinstance(outputs, tuple) else outputs\n",
    "        x = outputs\n",
    "        #print(\"compute loss!\", x[0].shape, self.no)\n",
    "        output = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n",
    "        pred_output, pred_scores = output.split((4 * self.dfl_ch, self.nc), 1)\n",
    "\n",
    "        pred_output = pred_output.permute(0, 2, 1).contiguous()\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        size = torch.tensor(x[0].shape[2:], dtype=pred_scores.dtype, device=self.device)\n",
    "        size = size * self.stride[0]\n",
    "\n",
    "        anchor_points, stride_tensor = make_anchors(x, self.stride, 0.5)\n",
    "\n",
    "        # targets\n",
    "        if targets.shape[0] == 0:\n",
    "            gt = torch.zeros(pred_scores.shape[0], 0, 5, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]  # image index\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            gt = torch.zeros(pred_scores.shape[0], counts.max(), 5, device=self.device)\n",
    "            for j in range(pred_scores.shape[0]):\n",
    "                matches = i == j\n",
    "                n = matches.sum()\n",
    "                if n:\n",
    "                    gt[j, :n] = targets[matches, 1:]\n",
    "            gt[..., 1:5] = wh2xy_ut(gt[..., 1:5].mul_(size[[1, 0, 1, 0]]))\n",
    "\n",
    "        gt_labels, gt_bboxes = gt.split((1, 4), 2)  # cls, xyxy\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
    "\n",
    "        # boxes\n",
    "        b, a, c = pred_output.shape\n",
    "        pred_bboxes = pred_output.view(b, a, 4, c // 4).softmax(3)\n",
    "        pred_bboxes = pred_bboxes.matmul(self.project.type(pred_bboxes.dtype))\n",
    "\n",
    "        a, b = torch.split(pred_bboxes, 2, -1)\n",
    "        pred_bboxes = torch.cat((anchor_points - a, anchor_points + b), -1)\n",
    "\n",
    "        scores = pred_scores.detach().sigmoid()\n",
    "        bboxes = (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype)\n",
    "        \n",
    "        #print(\"just outputs\", outputs[0].shape, pred_output.shape, targets.shape)\n",
    "        #print(\"into assign\", scores.shape,bboxes.shape, gt_labels.shape, mask_gt.shape, \n",
    "        #anchor_points.shape, stride_tensor.shape)\n",
    "        #pred_scores_cpu = pred_scores.cpu()\n",
    "        #pred_bboxes_cpu = pred_bboxes.cpu()\n",
    "        #gt_labels_cpu = gt_labels.cpu()\n",
    "        #gt_bboxes_cpu = gt_bboxes.cpu()\n",
    "        #mask_gt_cpu = mask_gt.cpu()\n",
    "        #anchor_points_cpu = anchor_points.cpu()\n",
    "        #stride_tensor_cpu = stride_tensor.cpu()\n",
    "        #assigned_values = self.assign(pred_scores_cpu, pred_bboxes_cpu, gt_labels_cpu, gt_bboxes_cpu, mask_gt_cpu, anchor_points_cpu * stride_tensor_cpu)\n",
    "#\n",
    "        \n",
    "        assign_val = self.assign(scores, bboxes,gt_labels, gt_bboxes, mask_gt,\n",
    "                                                            anchor_points * stride_tensor)\n",
    "        target_bboxes, target_scores, fg_mask, *yuck = assign_val\n",
    "\n",
    "        #print(\"after\", target_bboxes.shape, target_scores.shape, fg_mask.shape,       yuck)\n",
    "        target_bboxes /= stride_tensor\n",
    "        target_scores_sum = target_scores.sum()\n",
    "\n",
    "        # cls loss\n",
    "        loss_cls = self.bce(pred_scores, target_scores.to(pred_scores.dtype))\n",
    "        loss_cls = loss_cls.sum() / target_scores_sum\n",
    "\n",
    "        # box loss\n",
    "        loss_box = torch.zeros(1, device=self.device)\n",
    "        loss_dfl = torch.zeros(1, device=self.device)\n",
    "        if fg_mask.sum():\n",
    "            # IoU loss\n",
    "            weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
    "            loss_box = self.iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n",
    "            loss_box = ((1.0 - loss_box) * weight).sum() / target_scores_sum\n",
    "            # DFL loss\n",
    "            a, b = torch.split(target_bboxes, 2, -1)\n",
    "            target_lt_rb = torch.cat((anchor_points - a, b - anchor_points), -1)\n",
    "            target_lt_rb = target_lt_rb.clamp(0, self.dfl_ch - 1.01)  # distance (left_top, right_bottom)\n",
    "            loss_dfl = self.df_loss(pred_output[fg_mask].view(-1, self.dfl_ch), target_lt_rb[fg_mask])\n",
    "            loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n",
    "\n",
    "        #loss_cls *= self.params['cls']\n",
    "        #loss_box *= self.params['box']\n",
    "        #loss_dfl *= self.params['dfl']\n",
    "        return loss_cls + loss_box + loss_dfl  # loss(cls, box, dfl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def assign(self, pred_scores, pred_bboxes, true_labels, true_bboxes, true_mask, anchors):\n",
    "        \"\"\"\n",
    "        Task-aligned One-stage Object Detection assigner\n",
    "        \"\"\"\n",
    "        self.bs = pred_scores.size(0)\n",
    "        self.num_max_boxes = true_bboxes.size(1)\n",
    "\n",
    "        if self.num_max_boxes == 0:\n",
    "#            print('TRUE')\n",
    "            device = true_bboxes.device\n",
    "            return (torch.zeros_like(pred_bboxes).to(device),\n",
    "                    torch.zeros_like(pred_scores).to(device),\n",
    "                    torch.full_like(pred_scores[..., 0], self.nc).to(device).bool(),\n",
    "                    torch.zeros_like(pred_scores[..., 0]).to(device),\n",
    "                    torch.zeros_like(pred_scores[..., 0]).to(device))\n",
    "\n",
    "        i = torch.zeros([2, self.bs, self.num_max_boxes], dtype=torch.long)\n",
    "        i[0] = torch.arange(end=self.bs).view(-1, 1).repeat(1, self.num_max_boxes)\n",
    "        i[1] = true_labels.long().squeeze(-1)\n",
    "\n",
    "        overlaps = self.iou(true_bboxes.unsqueeze(2), pred_bboxes.unsqueeze(1))\n",
    "        overlaps = overlaps.squeeze(3).clamp(0)\n",
    "\n",
    "\n",
    "        #print(\"i shape\", i[0].shape, i[1].shape, pred_scores.shape)\n",
    "        #print(\"actual i[0] value\", i[0])\n",
    "        #print(\"actual i[1] value\", i[1])\n",
    "        #print(\"alignmetric\", pred_scores[i[0], :, i[1]].pow(self.alpha).shape)\n",
    "        #print( \"overlaps\", overlaps.pow(self.beta).shape)\n",
    "\n",
    "\n",
    "        align_metric = pred_scores[i[0], :, i[1]].pow(self.alpha) * overlaps.pow(self.beta)\n",
    "        bs, n_boxes, _ = true_bboxes.shape\n",
    "        lt, rb = true_bboxes.view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\n",
    "        bbox_deltas = torch.cat((anchors[None] - lt, rb - anchors[None]), dim=2)\n",
    "        mask_in_gts = bbox_deltas.view(bs, n_boxes, anchors.shape[0], -1).amin(3).gt_(1e-9)\n",
    "        metrics = align_metric * mask_in_gts\n",
    "        top_k_mask = true_mask.repeat([1, 1, self.top_k]).bool()\n",
    "        num_anchors = metrics.shape[-1]\n",
    "        top_k_metrics, top_k_indices = torch.topk(metrics, self.top_k, dim=-1, largest=True)\n",
    "        if top_k_mask is None:\n",
    "            top_k_mask = (top_k_metrics.max(-1, keepdim=True) > self.eps).tile([1, 1, self.top_k])\n",
    "        top_k_indices = torch.where(top_k_mask, top_k_indices, 0)\n",
    "        is_in_top_k = one_hot(top_k_indices, num_anchors).sum(-2)\n",
    "        # filter invalid boxes\n",
    "        is_in_top_k = torch.where(is_in_top_k > 1, 0, is_in_top_k)\n",
    "        mask_top_k = is_in_top_k.to(metrics.dtype)\n",
    "        # merge all mask to a final mask, (b, max_num_obj, h*w)\n",
    "        mask_pos = mask_top_k * mask_in_gts * true_mask\n",
    "\n",
    "        fg_mask = mask_pos.sum(-2)\n",
    "        if fg_mask.max() > 1:  # one anchor is assigned to multiple gt_bboxes\n",
    "            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).repeat([1, self.num_max_boxes, 1])\n",
    "            max_overlaps_idx = overlaps.argmax(1)\n",
    "            is_max_overlaps = one_hot(max_overlaps_idx, self.num_max_boxes)\n",
    "            is_max_overlaps = is_max_overlaps.permute(0, 2, 1).to(overlaps.dtype)\n",
    "            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos)\n",
    "            fg_mask = mask_pos.sum(-2)\n",
    "        # find each grid serve which gt(index)\n",
    "        target_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\n",
    "\n",
    "        # assigned target labels, (b, 1)\n",
    "        batch_index = torch.arange(end=self.bs,\n",
    "                                   dtype=torch.int64,\n",
    "                                   device=true_labels.device)[..., None]\n",
    "        target_gt_idx = target_gt_idx + batch_index * self.num_max_boxes\n",
    "        target_labels = true_labels.long().flatten()[target_gt_idx]\n",
    "\n",
    "        # assigned target boxes\n",
    "        target_bboxes = true_bboxes.view(-1, 4)[target_gt_idx]\n",
    "\n",
    "        # assigned target scores\n",
    "        target_labels.clamp(0)\n",
    "        target_scores = one_hot(target_labels, self.nc)\n",
    "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n",
    "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
    "\n",
    "        # normalize\n",
    "        align_metric *= mask_pos\n",
    "        pos_align_metrics = align_metric.amax(axis=-1, keepdim=True)\n",
    "        pos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)\n",
    "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2)\n",
    "        norm_align_metric = norm_align_metric.unsqueeze(-1)\n",
    "        target_scores = target_scores * norm_align_metric\n",
    "\n",
    "        #print(\"before\", target_bboxes.shape, target_scores.shape, fg_mask.bool().shape)\n",
    "        return target_bboxes, target_scores, fg_mask.bool()\n",
    "\n",
    "    @staticmethod\n",
    "    def df_loss(pred_dist, target):\n",
    "        # Return sum of left and right DFL losses\n",
    "        # Distribution Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "        tl = target.long()  # target left\n",
    "        tr = tl + 1  # target right\n",
    "        wl = tr - target  # weight left\n",
    "        wr = 1 - wl  # weight right\n",
    "        l_loss = cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape)\n",
    "        r_loss = cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape)\n",
    "        return (l_loss * wl + r_loss * wr).mean(-1, keepdim=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(box1, box2, eps=1e-7):\n",
    "        # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
    "\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "\n",
    "        # Intersection area\n",
    "        area1 = b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)\n",
    "        area2 = b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)\n",
    "        intersection = area1.clamp(0) * area2.clamp(0)\n",
    "\n",
    "        # Union Area\n",
    "        union = w1 * h1 + w2 * h2 - intersection + eps\n",
    "\n",
    "        # IoU\n",
    "        iou = intersection / union\n",
    "        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex width\n",
    "        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
    "        # Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "        c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "        # center dist ** 2\n",
    "        rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n",
    "        # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "        v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
    "        with torch.no_grad():\n",
    "            alpha = v / (v - iou + (1 + eps))\n",
    "        return iou - (rho2 / c2 + v * alpha)  # CIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train this fker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch.utils import data\n",
    "from scripts import util\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(args, params, model=None, filename_path=\"weapon-dataset/images/train/\"):\n",
    "\n",
    "\n",
    "    dataset = Dataset(filename_path, args['input_size'], params, False)\n",
    "    loader = data.DataLoader(dataset, 8, False, num_workers=8,\n",
    "                             pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "    if model is None:\n",
    "        model = torch.load('./weights/best.pt', map_location='cuda')['model'].float()\n",
    "\n",
    "    model.half()\n",
    "    model.eval()\n",
    "\n",
    "    # Configure\n",
    "    iou_v = torch.linspace(0.5, 0.95, 10).cuda()  # iou vector for mAP@0.5:0.95\n",
    "    n_iou = iou_v.numel()\n",
    "\n",
    "    m_pre = 0.\n",
    "    m_rec = 0.\n",
    "    map50 = 0.\n",
    "    mean_ap = 0.\n",
    "    metrics = []\n",
    "    p_bar = tqdm.tqdm(loader, desc=('%10s' * 3) % ('precision', 'recall', 'mAP'))\n",
    "    \n",
    "    for samples, targets, shapes in p_bar:\n",
    "        samples = samples.cuda()\n",
    "        targets = targets.cuda()\n",
    "        samples = samples.half()  # uint8 to fp16/32\n",
    "        samples = samples / 255  # 0 - 255 to 0.0 - 1.0\n",
    "        _, _, height, width = samples.shape  # batch size, channels, height, width\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(samples)\n",
    "\n",
    "        # NMS\n",
    "        targets[:, 2:] *= torch.tensor((width, height, width, height)).cuda()  # to pixels\n",
    "        outputs = util.non_max_suppression(outputs, 0.001, 0.65)\n",
    "\n",
    "        # Metrics\n",
    "        for i, output in enumerate(outputs):\n",
    "            labels = targets[targets[:, 0] == i, 1:]\n",
    "            correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).cuda()\n",
    "\n",
    "            if output.shape[0] == 0:\n",
    "                if labels.shape[0]:\n",
    "                    metrics.append((correct, *torch.zeros((3, 0)).cuda()))\n",
    "                continue\n",
    "\n",
    "            detections = output.clone()\n",
    "            util.scale(detections[:, :4], samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "            # Evaluate\n",
    "            if labels.shape[0]:\n",
    "                tbox = labels[:, 1:5].clone()  # target boxes\n",
    "                tbox[:, 0] = labels[:, 1] - labels[:, 3] / 2  # top left x\n",
    "                tbox[:, 1] = labels[:, 2] - labels[:, 4] / 2  # top left y\n",
    "                tbox[:, 2] = labels[:, 1] + labels[:, 3] / 2  # bottom right x\n",
    "                tbox[:, 3] = labels[:, 2] + labels[:, 4] / 2  # bottom right y\n",
    "                util.scale(tbox, samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "                correct = numpy.zeros((detections.shape[0], iou_v.shape[0]))\n",
    "                correct = correct.astype(bool)\n",
    "\n",
    "                t_tensor = torch.cat((labels[:, 0:1], tbox), 1)\n",
    "                iou = util.box_iou(t_tensor[:, 1:], detections[:, :4])\n",
    "                correct_class = t_tensor[:, 0:1] == detections[:, 5]\n",
    "                for j in range(len(iou_v)):\n",
    "                    x = torch.where((iou >= iou_v[j]) & correct_class)\n",
    "                    if x[0].shape[0]:\n",
    "                        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1)\n",
    "                        matches = matches.cpu().numpy()\n",
    "                        if x[0].shape[0] > 1:\n",
    "                            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                            matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n",
    "                            matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n",
    "                        correct[matches[:, 1].astype(int), j] = True\n",
    "                correct = torch.tensor(correct, dtype=torch.bool, device=iou_v.device)\n",
    "            metrics.append((correct, output[:, 4], output[:, 5], labels[:, 0]))\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = [torch.cat(x, 0).cpu().numpy() for x in zip(*metrics)]  # to numpy\n",
    "    if len(metrics) and metrics[0].any():\n",
    "        tp, fp, m_pre, m_rec, map50, mean_ap = util.compute_ap(*metrics)\n",
    "\n",
    "    # Print results\n",
    "    print('%10.3g' * 3 % (m_pre, m_rec, mean_ap))\n",
    "\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    return map50, mean_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch.utils import data\n",
    "from scripts import util\n",
    "\n",
    "def learning_rate(args,params):\n",
    "    def fn(x):\n",
    "        return (1 - x / args['epochs']) * (1.0 - params['lrf']) + params['lrf']\n",
    "    return fn\n",
    "\n",
    "losses_memory = []\n",
    "def train(args, params, filename_path, sequential=False):\n",
    "    # Model\n",
    "    #print(\"num classes\", len(params['names'].values()))\n",
    "    model = yolo_mamba_nano(len(params['names'].values()), n_layers_mamba=6).cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    accumulate = max(round(64 / (args['batch_size'] * args['world_size'])), 1)\n",
    "    params['weight_decay'] *= args['batch_size'] * args['world_size'] * accumulate / 64\n",
    "\n",
    "    p = [], [], []\n",
    "    for v in model.modules():\n",
    "        if hasattr(v, 'bias') and isinstance(v.bias, torch.nn.Parameter):\n",
    "            p[2].append(v.bias)\n",
    "        if isinstance(v, torch.nn.BatchNorm2d):\n",
    "            p[1].append(v.weight)\n",
    "        elif hasattr(v, 'weight') and isinstance(v.weight, torch.nn.Parameter):\n",
    "            p[0].append(v.weight)\n",
    "\n",
    "    optimizer = torch.optim.SGD(p[2], params['lr0'], params['momentum'], nesterov=True)\n",
    "\n",
    "    optimizer.add_param_group({'params': p[0], 'weight_decay': params['weight_decay']})\n",
    "    optimizer.add_param_group({'params': p[1]})\n",
    "    del p\n",
    "\n",
    "    # Scheduler\n",
    "    lr = learning_rate(args,params)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr, last_epoch=-1)\n",
    "\n",
    "    # EMA\n",
    "    ema = util.EMA(model) if args['local_rank'] == 0 else None\n",
    "\n",
    "    #filenames = os.listdir(filename_path)\n",
    "    #filenames = [filename_path + fname for fname in filenames if fname.endswith((\".jpg\", \".png\"))]\n",
    "    dataset = Dataset(filename_path, args['input_size'], params, True)\n",
    "\n",
    "    if args['world_size'] <= 1:\n",
    "        sampler = None\n",
    "    else:\n",
    "        sampler = data.distributed.DistributedSampler(dataset)\n",
    "\n",
    "    loader = data.DataLoader(dataset, args['batch_size'], sampler is None, sampler,collate_fn=Dataset.collate_fn, \n",
    "                             num_workers=8, pin_memory=True)\n",
    "\n",
    "    if args['world_size'] > 1:\n",
    "        # DDP mode\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        model = torch.nn.parallel.DistributedDataParallel(module=model,\n",
    "                                                          device_ids=[args['local_rank']],\n",
    "                                                          output_device=args['local_rank'])\n",
    "\n",
    "    # Start training\n",
    "    best = 0\n",
    "    num_batch = len(loader)\n",
    "    amp_scale = torch.cuda.amp.GradScaler()\n",
    "    criterion = ComputeLoss(model, params)\n",
    "    num_warmup = max(round(params['warmup_epochs'] * num_batch), 1000)\n",
    "    with open('scripts/step.csv', 'w') as f:\n",
    "        if args['local_rank'] == 0:\n",
    "            writer = csv.DictWriter(f, fieldnames=['epoch', 'mAP@50', 'mAP'])\n",
    "            writer.writeheader()\n",
    "        for epoch in range(args['epochs']):\n",
    "            model.train()\n",
    "\n",
    "            if args['epochs'] - epoch == 10:\n",
    "                loader.dataset.mosaic = False\n",
    "\n",
    "            m_loss = util.AverageMeter()\n",
    "            if args['world_size'] > 1:\n",
    "                sampler.set_epoch(epoch)\n",
    "            p_bar = enumerate(loader)\n",
    "            if args['local_rank'] == 0:\n",
    "                print(('\\n' + '%10s' * 3) % ('epoch', 'memory', 'loss'))\n",
    "            if args['local_rank'] == 0:\n",
    "                p_bar = tqdm.tqdm(p_bar, total=num_batch)  # progress bar\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            ep_loss = []\n",
    "            for i, (samples, targets, _) in p_bar:\n",
    "                x = i + num_batch * epoch  # number of iterations\n",
    "\n",
    "                samples = samples.cuda().float() / 255\n",
    "                targets = targets.cuda()\n",
    "\n",
    "                # Warmup\n",
    "                if x <= num_warmup:\n",
    "                    xp = [0, num_warmup]\n",
    "                    fp = [1, 64 / (args['batch_size'] * args['world_size'])]\n",
    "                    accumulate = max(1, numpy.interp(x, xp, fp).round())\n",
    "                    for j, y in enumerate(optimizer.param_groups):\n",
    "                        if j == 0:\n",
    "                            fp = [params['warmup_bias_lr'], y['initial_lr'] * lr(epoch)]\n",
    "                        else:\n",
    "                            fp = [0.0, y['initial_lr'] * lr(epoch)]\n",
    "                        y['lr'] = numpy.interp(x, xp, fp)\n",
    "                        if 'momentum' in y:\n",
    "                            fp = [params['warmup_momentum'], params['momentum']]\n",
    "                            y['momentum'] = numpy.interp(x, xp, fp)\n",
    "\n",
    "                # Forward\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(samples)  # forward\n",
    "                #print(outputs[0].shape, outputs[1].shape, outputs[2].shape, targets.shape)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                losses_memory.append(loss.item())\n",
    "                m_loss.update(loss.item(), samples.size(0))\n",
    "                \n",
    "                loss *= args['batch_size']  # loss scaled by batch_size\n",
    "                loss *= args['world_size']  # gradient averaged between devices in DDP mode\n",
    "\n",
    "                # Backward\n",
    "                #print(\"before scale\", loss.item())\n",
    "                #print(\"after scale\", loss.item())\n",
    "                amp_scale.scale(loss).backward()\n",
    "\n",
    "                # Optimize\n",
    "                if x % accumulate == 0:\n",
    "                    amp_scale.unscale_(optimizer)  # unscale gradients\n",
    "                    util.clip_gradients(model)  # clip gradients\n",
    "                    amp_scale.step(optimizer)  # optimizer.step\n",
    "                    amp_scale.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    if ema:\n",
    "                        ema.update(model)\n",
    "\n",
    "                # Log\n",
    "                if args['local_rank'] == 0:\n",
    "                    memory = f'{torch.cuda.memory_reserved() / 1E9:.3g}G'  # (GB)\n",
    "                    s = ('%10s' * 2 + '%10.4g') % (f'{epoch + 1}/{args[\"epochs\"]}', memory, m_loss.avg)\n",
    "                    p_bar.set_description(s)\n",
    "\n",
    "                del loss\n",
    "                del outputs\n",
    "\n",
    "            # Scheduler\n",
    "            scheduler.step()\n",
    "            #losses_memory.append(np.mean(ep_loss))\n",
    "            if args['local_rank'] == 0:\n",
    "                # mAP\n",
    "                last = test(args,['params'], ema.ema)\n",
    "                writer.writerow({'mAP': str(f'{last[1]:.3f}'),\n",
    "                                 'epoch': str(epoch + 1).zfill(3),\n",
    "                                 'mAP@50': str(f'{last[0]:.3f}')})\n",
    "                f.flush()\n",
    "\n",
    "                # Update best mAP\n",
    "                if last[1] > best:\n",
    "                    best = last[1]\n",
    "\n",
    "                ## Save model\n",
    "                #ckpt = {'model': copy.deepcopy(ema.ema).half()}\n",
    "#\n",
    "                ## Save last, best and delete\n",
    "                #torch.save(ckpt, './weights/last.pt')\n",
    "                #if best == last[1]:\n",
    "                #    torch.save(ckpt, './weights/best.pt')\n",
    "                #del ckpt\n",
    "\n",
    "    #if args['local_rank'] == 0:\n",
    "    #    util.strip_optimizer('./weights/best.pt')  # strip optimizers\n",
    "    #    util.strip_optimizer('./weights/last.pt')  # strip optimizers\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execute main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     1/100    0.449G  2.43e+07: 100%|██████████| 1739/1739 [00:47<00:00, 36.73it/s]\n",
      " precision    recall       mAP: 100%|██████████| 435/435 [00:05<00:00, 81.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         0         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     2/100    0.449G     15.95: 100%|██████████| 1739/1739 [00:44<00:00, 39.28it/s]\n",
      " precision    recall       mAP: 100%|██████████| 435/435 [00:05<00:00, 84.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         0         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     3/100    0.449G     17.17:  28%|██▊       | 495/1739 [00:11<00:30, 41.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         train(args, params, filename_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweapon-dataset/images/train/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#if args['test']:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#    test(args, params)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlrf\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweapon-dataset/images/train/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 122\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, params, filename_path, sequential)\u001b[0m\n\u001b[1;32m    120\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(samples)  \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#print(outputs[0].shape, outputs[1].shape, outputs[2].shape, targets.shape)\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m losses_memory\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    125\u001b[0m m_loss\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), samples\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[0;32mIn[10], line 86\u001b[0m, in \u001b[0;36mComputeLoss.__call__\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     71\u001b[0m         bboxes \u001b[38;5;241m=\u001b[39m (pred_bboxes\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m*\u001b[39m stride_tensor)\u001b[38;5;241m.\u001b[39mtype(gt_bboxes\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;66;03m#print(\"just outputs\", outputs[0].shape, pred_output.shape, targets.shape)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m#print(\"into assign\", scores.shape,bboxes.shape, gt_labels.shape, mask_gt.shape, \u001b[39;00m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m#anchor_points.shape, stride_tensor.shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;66;03m#assigned_values = self.assign(pred_scores_cpu, pred_bboxes_cpu, gt_labels_cpu, gt_bboxes_cpu, mask_gt_cpu, anchor_points_cpu * stride_tensor_cpu)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m         assign_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m         target_bboxes, target_scores, fg_mask, \u001b[38;5;241m*\u001b[39myuck \u001b[38;5;241m=\u001b[39m assign_val\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m#print(\"after\", target_bboxes.shape, target_scores.shape, fg_mask.shape,       yuck)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/louis/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 150\u001b[0m, in \u001b[0;36mComputeLoss.assign\u001b[0;34m(self, pred_scores, pred_bboxes, true_labels, true_bboxes, true_mask, anchors)\u001b[0m\n\u001b[1;32m    140\u001b[0m overlaps \u001b[38;5;241m=\u001b[39m overlaps\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m#print(\"i shape\", i[0].shape, i[1].shape, pred_scores.shape)\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m#print(\"actual i[0] value\", i[0])\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m#print(\"actual i[1] value\", i[1])\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m#print(\"alignmetric\", pred_scores[i[0], :, i[1]].pow(self.alpha).shape)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m#print( \"overlaps\", overlaps.pow(self.beta).shape)\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m align_metric \u001b[38;5;241m=\u001b[39m \u001b[43mpred_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m overlaps\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\n\u001b[1;32m    151\u001b[0m bs, n_boxes, _ \u001b[38;5;241m=\u001b[39m true_bboxes\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    152\u001b[0m lt, rb \u001b[38;5;241m=\u001b[39m true_bboxes\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# left-top, right-bottom\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = {\"input_size\": 640,\n",
    "            \"batch_size\": 2,\n",
    "            \"local_rank\": 0,\n",
    "            \"world_size\": 1,\n",
    "            \"epochs\": 100,\n",
    "            \"train\": True,\n",
    "            \"test\": True}\n",
    "\n",
    "    util.setup_seed()\n",
    "    util.setup_multi_processes()\n",
    "\n",
    "    with open(os.path.join('scripts', 'params.yaml'), errors='ignore') as f:\n",
    "        params = yaml.safe_load(f)\n",
    "\n",
    "    print(params['lrf'])\n",
    "    if args['train']:\n",
    "        train(args, params, filename_path=\"weapon-dataset/images/train/\")\n",
    "    #if args['test']:\n",
    "    #    test(args, params)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34.80146789550781,\n",
       " 112.66536712646484,\n",
       " 377.0284118652344,\n",
       " 541352.375,\n",
       " 40.746826171875,\n",
       " 306447.59375,\n",
       " 233103.59375,\n",
       " 84.76466369628906,\n",
       " 17.109642028808594,\n",
       " 15.045095443725586,\n",
       " 5059.87890625,\n",
       " 23.639440536499023,\n",
       " 19.163724899291992,\n",
       " 16.254663467407227,\n",
       " 39.946956634521484,\n",
       " 12.541455268859863,\n",
       " 186.1862335205078,\n",
       " 14.599713325500488,\n",
       " 532.7578735351562,\n",
       " 13.84423828125,\n",
       " 13.174128532409668,\n",
       " 12.568638801574707,\n",
       " 22.285316467285156,\n",
       " 16677.806640625,\n",
       " 16.413501739501953,\n",
       " nan,\n",
       " 21.88235092163086,\n",
       " 14.872879981994629,\n",
       " 14.517637252807617,\n",
       " 4.519803524017334,\n",
       " 14.939327239990234,\n",
       " 13.614545822143555,\n",
       " 15.44417667388916,\n",
       " 15.774690628051758,\n",
       " 16.613563537597656,\n",
       " 15.912320137023926,\n",
       " 15.91510009765625,\n",
       " 15.602724075317383,\n",
       " 15.24644660949707,\n",
       " 15.446088790893555,\n",
       " 15.024633407592773,\n",
       " 14.730128288269043,\n",
       " 3.6655936241149902,\n",
       " 14.8925199508667,\n",
       " 13.931751251220703,\n",
       " 14.95710563659668,\n",
       " nan,\n",
       " 14.919158935546875,\n",
       " 15.12222957611084,\n",
       " 4.388969421386719,\n",
       " 3.5461487770080566,\n",
       " 15.79442310333252,\n",
       " nan,\n",
       " 3.518397331237793,\n",
       " 16.67806625366211,\n",
       " 16.895238876342773,\n",
       " 17.383974075317383,\n",
       " 3.6597611904144287,\n",
       " 20.11248207092285,\n",
       " 17.56727409362793,\n",
       " 17.554386138916016,\n",
       " 15.931150436401367,\n",
       " 17.207435607910156,\n",
       " 3.6157443523406982,\n",
       " 16.559326171875,\n",
       " 18.339736938476562,\n",
       " 13.732768058776855,\n",
       " 18.180755615234375,\n",
       " 21.19524574279785,\n",
       " 17.872949600219727,\n",
       " 17.587739944458008,\n",
       " 17.904001235961914,\n",
       " 17.86024284362793,\n",
       " 17.275094985961914,\n",
       " 17.323638916015625,\n",
       " 16.682424545288086,\n",
       " 16.477970123291016,\n",
       " 18.314289093017578,\n",
       " 3.685655355453491,\n",
       " 17.45025062561035,\n",
       " 17.8936710357666,\n",
       " 17.2374267578125,\n",
       " 17.277631759643555,\n",
       " 16.794084548950195,\n",
       " 17.10330581665039,\n",
       " 14.69061279296875,\n",
       " 17.2803955078125,\n",
       " 18.218191146850586,\n",
       " 3.455504894256592,\n",
       " 17.765352249145508,\n",
       " 17.707000732421875,\n",
       " 17.619728088378906,\n",
       " 17.536270141601562,\n",
       " 15.628300666809082,\n",
       " 17.098346710205078,\n",
       " 17.57132911682129,\n",
       " 17.930423736572266,\n",
       " 17.92291259765625,\n",
       " 18.14168930053711,\n",
       " 17.273632049560547,\n",
       " 16.912437438964844,\n",
       " 17.36821746826172,\n",
       " 16.72235107421875,\n",
       " 16.83359718322754,\n",
       " 16.910215377807617,\n",
       " 15.657917022705078,\n",
       " 18.27092933654785,\n",
       " nan,\n",
       " nan,\n",
       " 16.829837799072266,\n",
       " 16.373374938964844,\n",
       " nan,\n",
       " 17.037473678588867,\n",
       " 16.192350387573242,\n",
       " 16.23470687866211,\n",
       " 16.37896728515625,\n",
       " 15.328058242797852,\n",
       " 16.401958465576172,\n",
       " 3.5795817375183105,\n",
       " 15.808926582336426,\n",
       " 15.37057876586914,\n",
       " 15.908709526062012,\n",
       " 15.69873332977295,\n",
       " 15.60356616973877,\n",
       " 16.720882415771484,\n",
       " 13.876620292663574,\n",
       " 16.027313232421875,\n",
       " 16.082738876342773,\n",
       " 15.688689231872559,\n",
       " 16.476585388183594,\n",
       " 16.313373565673828,\n",
       " 16.070035934448242,\n",
       " 15.425065994262695,\n",
       " 16.105207443237305,\n",
       " 15.289243698120117,\n",
       " 16.045148849487305,\n",
       " 16.318843841552734,\n",
       " 15.584039688110352,\n",
       " 16.180828094482422,\n",
       " 15.776113510131836,\n",
       " 15.760887145996094,\n",
       " nan,\n",
       " 15.765965461730957,\n",
       " 15.547471046447754,\n",
       " 3.301839828491211,\n",
       " 14.94883918762207,\n",
       " 15.982806205749512,\n",
       " 15.945052146911621,\n",
       " 15.109309196472168,\n",
       " 21.481508255004883,\n",
       " 15.201425552368164,\n",
       " nan,\n",
       " 15.244853019714355,\n",
       " 15.388688087463379,\n",
       " 15.264074325561523,\n",
       " 15.099591255187988,\n",
       " 15.392945289611816,\n",
       " 3.5656890869140625,\n",
       " 15.580232620239258,\n",
       " 15.236066818237305,\n",
       " 15.159276962280273,\n",
       " 14.43895149230957,\n",
       " 15.07421588897705,\n",
       " 14.392044067382812,\n",
       " 3.3591346740722656,\n",
       " 19.973527908325195,\n",
       " 14.54397964477539,\n",
       " 15.9308500289917,\n",
       " 3.2328543663024902,\n",
       " 16.686233520507812,\n",
       " 17.36910057067871,\n",
       " 3.2664053440093994,\n",
       " 15.064404487609863,\n",
       " 3.3722333908081055,\n",
       " 17.09490203857422,\n",
       " 14.917947769165039,\n",
       " 3.238368511199951,\n",
       " nan,\n",
       " 20.01444435119629,\n",
       " 20.53736686706543,\n",
       " 21.327980041503906,\n",
       " 19.645471572875977,\n",
       " 19.183401107788086,\n",
       " 20.639352798461914,\n",
       " 20.085121154785156,\n",
       " 20.344898223876953,\n",
       " 20.519588470458984,\n",
       " 20.588497161865234,\n",
       " 20.24005889892578,\n",
       " 20.768707275390625,\n",
       " 20.254547119140625,\n",
       " 3.2701351642608643,\n",
       " 20.54018211364746,\n",
       " 3.9033403396606445,\n",
       " 20.60602378845215,\n",
       " 20.088834762573242,\n",
       " 20.585845947265625,\n",
       " 20.328832626342773,\n",
       " 3.7061479091644287,\n",
       " 20.484718322753906,\n",
       " nan,\n",
       " nan,\n",
       " 3.725627899169922,\n",
       " 19.540525436401367,\n",
       " 3.4051597118377686,\n",
       " 3.685737133026123,\n",
       " 3.771921396255493,\n",
       " 20.15827178955078,\n",
       " 19.9364013671875,\n",
       " 20.61127471923828,\n",
       " 20.356704711914062,\n",
       " 19.8442325592041,\n",
       " 20.53525161743164,\n",
       " 19.998046875,\n",
       " 3.9254395961761475,\n",
       " 20.52289581298828,\n",
       " 20.159896850585938,\n",
       " 3.6087327003479004,\n",
       " 3.2902064323425293,\n",
       " 4.427448272705078,\n",
       " 20.300586700439453,\n",
       " 3.573288679122925,\n",
       " 20.206396102905273,\n",
       " 20.274024963378906,\n",
       " 20.251365661621094,\n",
       " 19.173198699951172,\n",
       " 20.31871795654297,\n",
       " 3.4150876998901367,\n",
       " 15.195718765258789,\n",
       " 19.574495315551758,\n",
       " 20.64597511291504,\n",
       " nan,\n",
       " 14.260433197021484,\n",
       " 3.921874523162842,\n",
       " 20.52506446838379,\n",
       " 20.34625816345215,\n",
       " 13.519023895263672,\n",
       " 20.43600082397461,\n",
       " 19.599624633789062,\n",
       " 3.4349589347839355,\n",
       " 20.220645904541016,\n",
       " 3.4095685482025146,\n",
       " 3.642268419265747,\n",
       " 20.577442169189453,\n",
       " 19.11155891418457,\n",
       " 20.14117431640625,\n",
       " 3.4268786907196045,\n",
       " 19.603290557861328,\n",
       " 20.972749710083008,\n",
       " 52.870948791503906,\n",
       " 20.971729278564453,\n",
       " 20.79355812072754,\n",
       " 3.254256010055542,\n",
       " 18.724777221679688,\n",
       " 20.58547019958496,\n",
       " nan,\n",
       " 37.300540924072266,\n",
       " 18.993898391723633,\n",
       " 3.6915767192840576,\n",
       " 20.349716186523438,\n",
       " 3.3535420894622803,\n",
       " 20.136686325073242,\n",
       " 20.297685623168945,\n",
       " nan,\n",
       " 20.56374740600586,\n",
       " nan,\n",
       " 20.076265335083008,\n",
       " nan,\n",
       " 20.9094295501709,\n",
       " 19.7672176361084,\n",
       " 20.45606231689453,\n",
       " 3.4247140884399414,\n",
       " 3.26430606842041,\n",
       " 19.971546173095703,\n",
       " 20.406883239746094,\n",
       " 19.899995803833008,\n",
       " 38.74116516113281,\n",
       " 20.140121459960938,\n",
       " nan,\n",
       " 19.54958152770996,\n",
       " 20.460922241210938,\n",
       " 20.13155174255371,\n",
       " 19.709396362304688,\n",
       " 19.361831665039062,\n",
       " 3.0521812438964844,\n",
       " 19.971904754638672,\n",
       " 3.3587474822998047,\n",
       " nan,\n",
       " 20.27980613708496,\n",
       " 20.024715423583984,\n",
       " 20.51079559326172,\n",
       " nan,\n",
       " 3.081174373626709,\n",
       " 19.256662368774414,\n",
       " nan,\n",
       " 21.176464080810547,\n",
       " 20.41485023498535,\n",
       " 18.670740127563477,\n",
       " 19.90170669555664,\n",
       " 19.929645538330078,\n",
       " 3.219790458679199,\n",
       " 20.485692977905273,\n",
       " 19.495445251464844,\n",
       " nan,\n",
       " 19.664949417114258,\n",
       " nan,\n",
       " nan,\n",
       " 20.404428482055664,\n",
       " 19.79430389404297,\n",
       " 19.881404876708984,\n",
       " 19.95855712890625,\n",
       " 19.434391021728516,\n",
       " 19.579790115356445,\n",
       " 3.3465819358825684,\n",
       " 20.581296920776367,\n",
       " 3.514913320541382,\n",
       " nan,\n",
       " 19.541059494018555,\n",
       " 19.359882354736328,\n",
       " 3.75121808052063,\n",
       " 19.970705032348633,\n",
       " 18.534278869628906,\n",
       " 18.67525863647461,\n",
       " 20.541810989379883,\n",
       " 20.2081241607666,\n",
       " 19.86240005493164,\n",
       " 19.79558753967285,\n",
       " 20.01465606689453,\n",
       " 19.35915184020996,\n",
       " 3.2671151161193848,\n",
       " 18.472545623779297,\n",
       " 3.629875659942627,\n",
       " 17.576343536376953,\n",
       " 18.482860565185547,\n",
       " 17.70138931274414,\n",
       " 18.850345611572266,\n",
       " 18.30710792541504,\n",
       " 17.26491928100586,\n",
       " 16.805341720581055,\n",
       " 18.88255500793457,\n",
       " 17.722244262695312,\n",
       " 13.542362213134766,\n",
       " nan,\n",
       " 18.06483268737793,\n",
       " 16.452665328979492,\n",
       " 18.8333683013916,\n",
       " 18.640291213989258,\n",
       " 18.949031829833984,\n",
       " 18.268522262573242,\n",
       " 17.51486587524414,\n",
       " 17.29695701599121,\n",
       " 17.846553802490234,\n",
       " 15.151167869567871,\n",
       " 16.74081802368164,\n",
       " 3.0363125801086426,\n",
       " 18.75484848022461,\n",
       " 17.0737361907959,\n",
       " 18.65863800048828,\n",
       " 18.155603408813477,\n",
       " 19.084501266479492,\n",
       " 18.660057067871094,\n",
       " 17.9318790435791,\n",
       " 18.117788314819336,\n",
       " 17.977691650390625,\n",
       " 3.2757554054260254,\n",
       " 16.595327377319336,\n",
       " 18.55276107788086,\n",
       " 17.80992317199707,\n",
       " 16.951736450195312,\n",
       " 17.282739639282227,\n",
       " 18.1350040435791,\n",
       " 17.39029312133789,\n",
       " 16.713069915771484,\n",
       " 17.484569549560547,\n",
       " 9.853240013122559,\n",
       " 18.837081909179688,\n",
       " 17.151321411132812,\n",
       " 16.32940101623535,\n",
       " 16.96986198425293,\n",
       " 17.71708106994629,\n",
       " 18.410202026367188,\n",
       " 17.654804229736328,\n",
       " 16.060081481933594,\n",
       " 15.313097953796387,\n",
       " 18.0245361328125,\n",
       " 16.796527862548828,\n",
       " 15.515432357788086,\n",
       " 16.89611053466797,\n",
       " 17.35345458984375,\n",
       " 17.863496780395508,\n",
       " 16.037025451660156,\n",
       " 17.540632247924805,\n",
       " 15.41671085357666,\n",
       " 17.64956283569336,\n",
       " 17.290508270263672,\n",
       " 15.352999687194824,\n",
       " nan,\n",
       " 17.81245994567871,\n",
       " 17.62992286682129,\n",
       " 17.925756454467773,\n",
       " 16.54987907409668,\n",
       " 17.58036231994629,\n",
       " 15.532943725585938,\n",
       " 17.362558364868164,\n",
       " 28794505216.0,\n",
       " 16.885942459106445,\n",
       " 18.56026268005371,\n",
       " 3.204347610473633,\n",
       " nan,\n",
       " 17.413583755493164,\n",
       " 16.070985794067383,\n",
       " 16.386262893676758,\n",
       " 16.137977600097656,\n",
       " 14.198728561401367,\n",
       " 15.546428680419922,\n",
       " 18.426658630371094,\n",
       " nan,\n",
       " 3.1677517890930176,\n",
       " 15.142889976501465,\n",
       " 15.108168601989746,\n",
       " 17.421710968017578,\n",
       " 15.809098243713379,\n",
       " 16.678491592407227,\n",
       " 17.8404541015625,\n",
       " 3.4047482013702393,\n",
       " 17.691509246826172,\n",
       " 17.753707885742188,\n",
       " 16.510583877563477,\n",
       " 15.662555694580078,\n",
       " 3.4429962635040283,\n",
       " 15.942708969116211,\n",
       " 16.906536102294922,\n",
       " 17.274805068969727,\n",
       " 17.50662612915039,\n",
       " 16.212385177612305,\n",
       " 17.174724578857422,\n",
       " 14.650199890136719,\n",
       " 15.755191802978516,\n",
       " 13.232608795166016,\n",
       " 16.550060272216797,\n",
       " 16.383342742919922,\n",
       " 3.693882703781128,\n",
       " 17.11637306213379,\n",
       " 16.389619827270508,\n",
       " 17.281171798706055,\n",
       " 17.046905517578125,\n",
       " 16.233810424804688,\n",
       " 16.425161361694336,\n",
       " 15.812174797058105,\n",
       " 16.279682159423828,\n",
       " 16.846763610839844,\n",
       " 16.358200073242188,\n",
       " 16.374387741088867,\n",
       " 16.54483413696289,\n",
       " 16.880428314208984,\n",
       " 16.66901397705078,\n",
       " 17.690689086914062,\n",
       " 16.543128967285156,\n",
       " 16.156267166137695,\n",
       " 16.706148147583008,\n",
       " 16.910783767700195,\n",
       " 15.821107864379883,\n",
       " 17.49350357055664,\n",
       " 16.1981143951416,\n",
       " 18.214826583862305,\n",
       " nan,\n",
       " 17.156780242919922,\n",
       " 16.207256317138672,\n",
       " 17.05618667602539,\n",
       " 17.737367630004883,\n",
       " 16.606115341186523,\n",
       " 16.400497436523438,\n",
       " 16.475730895996094,\n",
       " 16.563858032226562,\n",
       " 16.785436630249023,\n",
       " 16.221092224121094,\n",
       " 17.054731369018555,\n",
       " 16.764280319213867,\n",
       " 17.486953735351562,\n",
       " 17.23358726501465,\n",
       " 17.242870330810547,\n",
       " 16.864452362060547,\n",
       " 16.4166202545166,\n",
       " 18.03046989440918,\n",
       " 18.03586769104004,\n",
       " nan,\n",
       " 17.38967514038086,\n",
       " 16.73299789428711,\n",
       " 17.166927337646484,\n",
       " 16.543561935424805,\n",
       " 16.82864761352539,\n",
       " 16.268600463867188,\n",
       " 16.540098190307617,\n",
       " 15.910804748535156,\n",
       " 17.496280670166016,\n",
       " 17.736310958862305,\n",
       " 16.672439575195312,\n",
       " 15.83525276184082,\n",
       " 3.1674413681030273,\n",
       " 17.42999267578125,\n",
       " 17.171506881713867,\n",
       " 17.025936126708984,\n",
       " 3.443213939666748,\n",
       " 17.761743545532227,\n",
       " 17.468032836914062,\n",
       " 17.121158599853516,\n",
       " 16.11620330810547,\n",
       " 3.5259804725646973,\n",
       " 3.2571704387664795,\n",
       " 17.39581298828125,\n",
       " 19.415485382080078,\n",
       " 16.87822914123535,\n",
       " 16.933048248291016,\n",
       " 17.261131286621094,\n",
       " 16.327011108398438,\n",
       " 16.946077346801758,\n",
       " 3.110898971557617,\n",
       " 16.48000144958496,\n",
       " 17.59880256652832,\n",
       " 17.511383056640625,\n",
       " 3.1196277141571045,\n",
       " 17.59339714050293,\n",
       " 17.808429718017578,\n",
       " 17.572132110595703,\n",
       " 18.04962921142578,\n",
       " 19.68756866455078,\n",
       " 17.334991455078125,\n",
       " 17.337255477905273,\n",
       " 18.061948776245117,\n",
       " 19.19078826904297,\n",
       " 17.958629608154297,\n",
       " 17.9354248046875,\n",
       " 18.465171813964844,\n",
       " nan,\n",
       " 17.904321670532227,\n",
       " 18.643470764160156,\n",
       " 3.227475643157959,\n",
       " 17.3355655670166,\n",
       " 16.91265869140625,\n",
       " 17.452739715576172,\n",
       " 17.424562454223633,\n",
       " 17.745325088500977,\n",
       " 17.636404037475586,\n",
       " 17.419673919677734,\n",
       " 17.488449096679688,\n",
       " 3.087893486022949,\n",
       " 17.940563201904297,\n",
       " 17.76871681213379,\n",
       " 17.727222442626953,\n",
       " 17.37090492248535,\n",
       " 17.664091110229492,\n",
       " 18.140562057495117,\n",
       " 18.10454559326172,\n",
       " 17.888442993164062,\n",
       " 18.12824821472168,\n",
       " nan,\n",
       " 17.91689682006836,\n",
       " 18.016586303710938,\n",
       " 17.64729881286621,\n",
       " 18.0848445892334,\n",
       " 17.445751190185547,\n",
       " 17.589954376220703,\n",
       " 18.224470138549805,\n",
       " 3.164308786392212,\n",
       " 16.659006118774414,\n",
       " 17.5812931060791,\n",
       " 18.482114791870117,\n",
       " 18.242063522338867,\n",
       " nan,\n",
       " 17.767457962036133,\n",
       " 3.370849609375,\n",
       " 17.996360778808594,\n",
       " 17.189271926879883,\n",
       " 17.654987335205078,\n",
       " 17.984081268310547,\n",
       " 18.988056182861328,\n",
       " 18.3033447265625,\n",
       " 18.50121307373047,\n",
       " 18.931093215942383,\n",
       " 3.109915256500244,\n",
       " 3.141730308532715,\n",
       " 20.65791130065918,\n",
       " 20.287900924682617,\n",
       " nan,\n",
       " 18.287548065185547,\n",
       " 19.20343780517578,\n",
       " 18.662721633911133,\n",
       " 25.54852294921875,\n",
       " 19.141620635986328,\n",
       " 19.009138107299805,\n",
       " 19.61905860900879,\n",
       " 23.04792594909668,\n",
       " 18.98822593688965,\n",
       " 36.88233947753906,\n",
       " 19.533044815063477,\n",
       " 20.33323097229004,\n",
       " 18.835317611694336,\n",
       " 19.294025421142578,\n",
       " 20.104610443115234,\n",
       " 19.82285499572754,\n",
       " 19.038436889648438,\n",
       " 28.95438575744629,\n",
       " 18.75542449951172,\n",
       " 20.651060104370117,\n",
       " 19.398229598999023,\n",
       " 19.432357788085938,\n",
       " 20.096981048583984,\n",
       " 19.55015754699707,\n",
       " 19.500381469726562,\n",
       " 19.906002044677734,\n",
       " 19.956615447998047,\n",
       " 19.08655548095703,\n",
       " 19.588024139404297,\n",
       " 20.768674850463867,\n",
       " 23.152423858642578,\n",
       " 19.39900779724121,\n",
       " 19.759397506713867,\n",
       " 19.582496643066406,\n",
       " 20.230627059936523,\n",
       " 3.5742897987365723,\n",
       " 37.90343475341797,\n",
       " 3.01033878326416,\n",
       " 3.3024230003356934,\n",
       " 20.03746223449707,\n",
       " 20.075546264648438,\n",
       " 19.89425277709961,\n",
       " 20.34969711303711,\n",
       " 20.135379791259766,\n",
       " 19.85342025756836,\n",
       " 20.35701560974121,\n",
       " 28.442270278930664,\n",
       " 20.77392578125,\n",
       " 20.19287109375,\n",
       " 20.882829666137695,\n",
       " 38.04789733886719,\n",
       " 20.033935546875,\n",
       " 20.101795196533203,\n",
       " 3.1166799068450928,\n",
       " 20.190723419189453,\n",
       " 20.463768005371094,\n",
       " 20.858875274658203,\n",
       " 19.875829696655273,\n",
       " 3.2727065086364746,\n",
       " nan,\n",
       " 19.62221336364746,\n",
       " 20.16501235961914,\n",
       " nan,\n",
       " 23.033306121826172,\n",
       " nan,\n",
       " 20.97724151611328,\n",
       " 21.191410064697266,\n",
       " 20.88857650756836,\n",
       " nan,\n",
       " 19.726337432861328,\n",
       " 20.00872039794922,\n",
       " 20.669647216796875,\n",
       " nan,\n",
       " 3.1674461364746094,\n",
       " 20.02602195739746,\n",
       " 3.147475004196167,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 20.013324737548828,\n",
       " 20.204177856445312,\n",
       " 20.239543914794922,\n",
       " 20.322444915771484,\n",
       " 20.071245193481445,\n",
       " 20.29541778564453,\n",
       " nan,\n",
       " nan,\n",
       " 20.92939567565918,\n",
       " 46.53429412841797,\n",
       " 21.107929229736328,\n",
       " 21.013729095458984,\n",
       " 20.373851776123047,\n",
       " nan,\n",
       " 20.009592056274414,\n",
       " 21.233407974243164,\n",
       " 3.8168652057647705,\n",
       " 20.350738525390625,\n",
       " nan,\n",
       " 3.2051591873168945,\n",
       " 21.148452758789062,\n",
       " 3.1570491790771484,\n",
       " 19.964427947998047,\n",
       " 20.628597259521484,\n",
       " 20.026748657226562,\n",
       " 3.110353469848633,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 21.05523109436035,\n",
       " nan,\n",
       " nan,\n",
       " 20.27079963684082,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3.388044595718384,\n",
       " 20.50153350830078,\n",
       " nan,\n",
       " 20.677738189697266,\n",
       " nan,\n",
       " 20.50605583190918,\n",
       " nan,\n",
       " 3.6637344360351562,\n",
       " 20.517822265625,\n",
       " 20.835729598999023,\n",
       " 20.899168014526367,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3.80770206451416,\n",
       " 20.817750930786133,\n",
       " 3.163421392440796,\n",
       " 4.058805465698242,\n",
       " 21.089067459106445,\n",
       " 21.026214599609375,\n",
       " 20.248260498046875,\n",
       " 20.716989517211914,\n",
       " 20.298036575317383,\n",
       " nan,\n",
       " 20.445907592773438,\n",
       " nan,\n",
       " nan,\n",
       " 20.746387481689453,\n",
       " nan,\n",
       " 20.404033660888672,\n",
       " nan,\n",
       " 3.2050814628601074,\n",
       " nan,\n",
       " 20.458240509033203,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 20.615983963012695,\n",
       " 20.533842086791992,\n",
       " nan,\n",
       " 3.024308681488037,\n",
       " 5.65945291519165,\n",
       " 23.739152908325195,\n",
       " 20.7142391204834,\n",
       " nan,\n",
       " nan,\n",
       " 20.541492462158203,\n",
       " 20.72875213623047,\n",
       " nan,\n",
       " 20.119831085205078,\n",
       " 20.5098819732666,\n",
       " 20.522525787353516,\n",
       " nan,\n",
       " nan,\n",
       " 22.430696487426758,\n",
       " nan,\n",
       " nan,\n",
       " 31.78978729248047,\n",
       " nan,\n",
       " 3.423703193664551,\n",
       " 20.124820709228516,\n",
       " 30.92654037475586,\n",
       " nan,\n",
       " 20.678874969482422,\n",
       " 20.11727523803711,\n",
       " nan,\n",
       " 21.126190185546875,\n",
       " 3.0719761848449707,\n",
       " 20.47046661376953,\n",
       " 3.3152029514312744,\n",
       " 19.406274795532227,\n",
       " 4.969329833984375,\n",
       " 21.74921417236328,\n",
       " nan,\n",
       " 22.529651641845703,\n",
       " 2.9196419715881348,\n",
       " 20.756546020507812,\n",
       " nan,\n",
       " 20.344627380371094,\n",
       " nan,\n",
       " 20.968074798583984,\n",
       " 3.280797004699707,\n",
       " nan,\n",
       " 19.95395278930664,\n",
       " nan,\n",
       " nan,\n",
       " 3.0206878185272217,\n",
       " 20.63854217529297,\n",
       " 22.439613342285156,\n",
       " 20.084314346313477,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3.902935028076172,\n",
       " 25.010637283325195,\n",
       " nan,\n",
       " 20.37891960144043,\n",
       " nan,\n",
       " nan,\n",
       " 20.14080047607422,\n",
       " 19.980083465576172,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 20.567636489868164,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 20.20164680480957,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 20.753002166748047,\n",
       " 20.33194351196289,\n",
       " 28.399646759033203,\n",
       " 20.73920440673828,\n",
       " 20.908491134643555,\n",
       " nan,\n",
       " 20.80946159362793,\n",
       " 20.86682891845703,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 20.900365829467773,\n",
       " 20.144786834716797,\n",
       " 20.293222427368164,\n",
       " 3.931396961212158,\n",
       " nan,\n",
       " nan,\n",
       " 18.598417282104492,\n",
       " 20.5831356048584,\n",
       " 20.686620712280273,\n",
       " nan,\n",
       " 20.582157135009766,\n",
       " 19.871889114379883,\n",
       " 4.72553014755249,\n",
       " 21.308019638061523,\n",
       " 19.721141815185547,\n",
       " 21.137554168701172,\n",
       " nan,\n",
       " nan,\n",
       " 20.269546508789062,\n",
       " 20.026395797729492,\n",
       " 21.134366989135742,\n",
       " nan,\n",
       " 20.29201889038086,\n",
       " 20.886066436767578,\n",
       " 20.445728302001953,\n",
       " 20.288002014160156,\n",
       " nan,\n",
       " 20.7680721282959,\n",
       " 20.443859100341797,\n",
       " nan,\n",
       " 20.341787338256836,\n",
       " 3.782090187072754,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 18.56195068359375,\n",
       " 4.003132343292236,\n",
       " nan,\n",
       " 21.5706787109375,\n",
       " 20.5804500579834,\n",
       " nan,\n",
       " 20.11345672607422,\n",
       " 3.3091788291931152,\n",
       " nan,\n",
       " 20.441469192504883,\n",
       " 20.341367721557617,\n",
       " nan,\n",
       " nan,\n",
       " 21.090744018554688,\n",
       " 20.389223098754883,\n",
       " 46.498435974121094,\n",
       " nan,\n",
       " nan,\n",
       " 20.18054962158203,\n",
       " 20.471420288085938,\n",
       " 20.700233459472656,\n",
       " 20.912643432617188,\n",
       " 20.643779754638672,\n",
       " 20.51416778564453,\n",
       " nan,\n",
       " nan,\n",
       " 3.3124377727508545,\n",
       " nan,\n",
       " 27.283199310302734,\n",
       " 20.384361267089844,\n",
       " 20.373403549194336,\n",
       " 20.263986587524414,\n",
       " 20.98411750793457,\n",
       " nan,\n",
       " 20.902313232421875,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 20.64059066772461,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 4.053178310394287,\n",
       " nan,\n",
       " 20.825056076049805,\n",
       " 20.219919204711914,\n",
       " 20.250980377197266,\n",
       " nan,\n",
       " 19.956687927246094,\n",
       " 4.115748405456543,\n",
       " 3.8844428062438965,\n",
       " 20.971694946289062,\n",
       " 19.22547721862793,\n",
       " 20.628999710083008,\n",
       " 20.488067626953125,\n",
       " nan,\n",
       " nan,\n",
       " 20.53409767150879,\n",
       " nan,\n",
       " nan,\n",
       " 20.551956176757812,\n",
       " nan,\n",
       " 20.51913070678711,\n",
       " nan,\n",
       " nan,\n",
       " 19.873666763305664,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 20.40550994873047,\n",
       " nan,\n",
       " 20.060131072998047,\n",
       " nan,\n",
       " 20.590770721435547,\n",
       " 20.74860382080078,\n",
       " 20.19573974609375,\n",
       " 20.837308883666992,\n",
       " 20.947948455810547,\n",
       " nan,\n",
       " 4.064767837524414,\n",
       " 20.341007232666016,\n",
       " 19.747028350830078,\n",
       " 67.11083221435547,\n",
       " 3.709583282470703,\n",
       " nan,\n",
       " 20.861560821533203,\n",
       " 20.42723846435547,\n",
       " 20.226951599121094,\n",
       " 20.561328887939453,\n",
       " 2.9790127277374268,\n",
       " 20.26824188232422,\n",
       " 21.289705276489258,\n",
       " 3.778769016265869,\n",
       " nan,\n",
       " nan,\n",
       " 21.491703033447266,\n",
       " nan,\n",
       " 3.7681984901428223,\n",
       " nan,\n",
       " nan,\n",
       " 19.63490104675293,\n",
       " nan,\n",
       " 20.301904678344727,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3.7629446983337402,\n",
       " 24.082500457763672,\n",
       " 20.124208450317383,\n",
       " 19.93672752380371,\n",
       " nan,\n",
       " nan,\n",
       " 20.481258392333984,\n",
       " nan,\n",
       " 20.138193130493164,\n",
       " nan,\n",
       " 20.088346481323242,\n",
       " nan,\n",
       " nan,\n",
       " 3.254486560821533,\n",
       " nan,\n",
       " nan,\n",
       " 3.283781051635742,\n",
       " 21.064603805541992,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 3.7637786865234375,\n",
       " nan,\n",
       " nan,\n",
       " 20.011581420898438,\n",
       " nan,\n",
       " nan,\n",
       " 20.822643280029297,\n",
       " nan,\n",
       " nan,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x714bdeb45120>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "series = pd.Series(losses_memory).dropna()\n",
    "\n",
    "# Calculate the rolling mean with a window of 500\n",
    "rolling_mean = series.rolling(window=100).mean()\n",
    "plt.plot(rolling_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '0'], dtype='<U1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_labels = []\n",
    "for f in os.listdir(\"weapon-dataset/labels/validation/\"):\n",
    "    diff_labels.append(open(\"weapon-dataset/labels/validation/\" + f).read().split(' ')[0])\n",
    "np.unique(diff_labels)\n",
    "# FUK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_class_numbers(directory):\n",
    "    # Get a list of all txt files in the directory\n",
    "    txt_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(directory, txt_file)\n",
    "        \n",
    "        # Read the contents of the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Modify the class number to 0 if it is not already 0\n",
    "        modified_lines = []\n",
    "        for line in lines:\n",
    "            if line.strip():  # Check if the line is not empty\n",
    "                parts = line.split()\n",
    "                if parts[0] != '0':\n",
    "                    parts[0] = '0'\n",
    "                modified_lines.append(' '.join(parts) + '\\n')\n",
    "\n",
    "        # Write the modified contents back to the file\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.writelines(modified_lines)\n",
    "\n",
    "directory_path = 'weapon-dataset/labels/train/'\n",
    "update_class_numbers(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test(args, params, model=None):\n",
    "    filenames = []\n",
    "    with open('../Dataset/COCO/val2017.txt') as reader:\n",
    "        for filename in reader.readlines():\n",
    "            filename = filename.rstrip().split('/')[-1]\n",
    "            filenames.append('../Dataset/COCO/images/val2017/' + filename)\n",
    "\n",
    "    dataset = Dataset(filenames, args.input_size, params, False)\n",
    "    loader = data.DataLoader(dataset, 8, False, num_workers=8,\n",
    "                             pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "    if model is None:\n",
    "        model = torch.load('./weights/best.pt', map_location='cuda')['model'].float()\n",
    "\n",
    "    model.half()\n",
    "    model.eval()\n",
    "\n",
    "    # Configure\n",
    "    iou_v = torch.linspace(0.5, 0.95, 10).cuda()  # iou vector for mAP@0.5:0.95\n",
    "    n_iou = iou_v.numel()\n",
    "\n",
    "    m_pre = 0.\n",
    "    m_rec = 0.\n",
    "    map50 = 0.\n",
    "    mean_ap = 0.\n",
    "    metrics = []\n",
    "    p_bar = tqdm.tqdm(loader, desc=('%10s' * 3) % ('precision', 'recall', 'mAP'))\n",
    "    for samples, targets, shapes in p_bar:\n",
    "        samples = samples.cuda()\n",
    "        targets = targets.cuda()\n",
    "        samples = samples.half()  # uint8 to fp16/32\n",
    "        samples = samples / 255  # 0 - 255 to 0.0 - 1.0\n",
    "        _, _, height, width = samples.shape  # batch size, channels, height, width\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(samples)\n",
    "\n",
    "        # NMS\n",
    "        targets[:, 2:] *= torch.tensor((width, height, width, height)).cuda()  # to pixels\n",
    "        outputs = util.non_max_suppression(outputs, 0.001, 0.65)\n",
    "\n",
    "        # Metrics\n",
    "        for i, output in enumerate(outputs):\n",
    "            labels = targets[targets[:, 0] == i, 1:]\n",
    "            correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).cuda()\n",
    "\n",
    "            if output.shape[0] == 0:\n",
    "                if labels.shape[0]:\n",
    "                    metrics.append((correct, *torch.zeros((3, 0)).cuda()))\n",
    "                continue\n",
    "\n",
    "            detections = output.clone()\n",
    "            util.scale(detections[:, :4], samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "            # Evaluate\n",
    "            if labels.shape[0]:\n",
    "                tbox = labels[:, 1:5].clone()  # target boxes\n",
    "                tbox[:, 0] = labels[:, 1] - labels[:, 3] / 2  # top left x\n",
    "                tbox[:, 1] = labels[:, 2] - labels[:, 4] / 2  # top left y\n",
    "                tbox[:, 2] = labels[:, 1] + labels[:, 3] / 2  # bottom right x\n",
    "                tbox[:, 3] = labels[:, 2] + labels[:, 4] / 2  # bottom right y\n",
    "                util.scale(tbox, samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "                correct = numpy.zeros((detections.shape[0], iou_v.shape[0]))\n",
    "                correct = correct.astype(bool)\n",
    "\n",
    "                t_tensor = torch.cat((labels[:, 0:1], tbox), 1)\n",
    "                iou = util.box_iou(t_tensor[:, 1:], detections[:, :4])\n",
    "                correct_class = t_tensor[:, 0:1] == detections[:, 5]\n",
    "                for j in range(len(iou_v)):\n",
    "                    x = torch.where((iou >= iou_v[j]) & correct_class)\n",
    "                    if x[0].shape[0]:\n",
    "                        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1)\n",
    "                        matches = matches.cpu().numpy()\n",
    "                        if x[0].shape[0] > 1:\n",
    "                            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                            matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n",
    "                            matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n",
    "                        correct[matches[:, 1].astype(int), j] = True\n",
    "                correct = torch.tensor(correct, dtype=torch.bool, device=iou_v.device)\n",
    "            metrics.append((correct, output[:, 4], output[:, 5], labels[:, 0]))\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = [torch.cat(x, 0).cpu().numpy() for x in zip(*metrics)]  # to numpy\n",
    "    if len(metrics) and metrics[0].any():\n",
    "        tp, fp, m_pre, m_rec, map50, mean_ap = util.compute_ap(*metrics)\n",
    "\n",
    "    # Print results\n",
    "    print('%10.3g' * 3 % (m_pre, m_rec, mean_ap))\n",
    "\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    return map50, mean_ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vimm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
